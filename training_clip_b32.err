Using backend: pytorch
2025-04-26 16:12:35,915 - INFO - config: configs/default_3dgres.yaml
2025-04-26 16:12:35,920 - INFO - set random seed: 1999
2025-04-26 16:12:39,052 - INFO - Parameters: 89.32M
2025-04-26 16:12:39,054 - INFO - Load pretrain from backbones/sp_unet_backbone.pth
2025-04-26 16:12:39,145 - WARNING - The model and loaded state dict do not match exactly

size mismatch for criterion.loss_weight: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([6]).
unexpected key in source state_dict: lang_proj.weight, lang_proj.bias, stm.x_mask.0.weight, stm.x_mask.0.bias, stm.x_mask.2.weight, stm.x_mask.2.bias

missing keys in source state_dict: bert_encoder.text_model.embeddings.token_embedding.weight, bert_encoder.text_model.embeddings.position_embedding.weight, bert_encoder.text_model.encoder.layers.0.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.0.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.0.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.0.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.0.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.0.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.0.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.0.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.0.layer_norm1.weight, bert_encoder.text_model.encoder.layers.0.layer_norm1.bias, bert_encoder.text_model.encoder.layers.0.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.0.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.0.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.0.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.0.layer_norm2.weight, bert_encoder.text_model.encoder.layers.0.layer_norm2.bias, bert_encoder.text_model.encoder.layers.1.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.1.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.1.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.1.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.1.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.1.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.1.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.1.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.1.layer_norm1.weight, bert_encoder.text_model.encoder.layers.1.layer_norm1.bias, bert_encoder.text_model.encoder.layers.1.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.1.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.1.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.1.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.1.layer_norm2.weight, bert_encoder.text_model.encoder.layers.1.layer_norm2.bias, bert_encoder.text_model.encoder.layers.2.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.2.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.2.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.2.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.2.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.2.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.2.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.2.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.2.layer_norm1.weight, bert_encoder.text_model.encoder.layers.2.layer_norm1.bias, bert_encoder.text_model.encoder.layers.2.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.2.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.2.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.2.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.2.layer_norm2.weight, bert_encoder.text_model.encoder.layers.2.layer_norm2.bias, bert_encoder.text_model.encoder.layers.3.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.3.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.3.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.3.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.3.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.3.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.3.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.3.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.3.layer_norm1.weight, bert_encoder.text_model.encoder.layers.3.layer_norm1.bias, bert_encoder.text_model.encoder.layers.3.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.3.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.3.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.3.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.3.layer_norm2.weight, bert_encoder.text_model.encoder.layers.3.layer_norm2.bias, bert_encoder.text_model.encoder.layers.4.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.4.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.4.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.4.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.4.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.4.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.4.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.4.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.4.layer_norm1.weight, bert_encoder.text_model.encoder.layers.4.layer_norm1.bias, bert_encoder.text_model.encoder.layers.4.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.4.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.4.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.4.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.4.layer_norm2.weight, bert_encoder.text_model.encoder.layers.4.layer_norm2.bias, bert_encoder.text_model.encoder.layers.5.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.5.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.5.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.5.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.5.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.5.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.5.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.5.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.5.layer_norm1.weight, bert_encoder.text_model.encoder.layers.5.layer_norm1.bias, bert_encoder.text_model.encoder.layers.5.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.5.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.5.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.5.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.5.layer_norm2.weight, bert_encoder.text_model.encoder.layers.5.layer_norm2.bias, bert_encoder.text_model.encoder.layers.6.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.6.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.6.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.6.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.6.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.6.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.6.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.6.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.6.layer_norm1.weight, bert_encoder.text_model.encoder.layers.6.layer_norm1.bias, bert_encoder.text_model.encoder.layers.6.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.6.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.6.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.6.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.6.layer_norm2.weight, bert_encoder.text_model.encoder.layers.6.layer_norm2.bias, bert_encoder.text_model.encoder.layers.7.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.7.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.7.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.7.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.7.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.7.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.7.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.7.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.7.layer_norm1.weight, bert_encoder.text_model.encoder.layers.7.layer_norm1.bias, bert_encoder.text_model.encoder.layers.7.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.7.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.7.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.7.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.7.layer_norm2.weight, bert_encoder.text_model.encoder.layers.7.layer_norm2.bias, bert_encoder.text_model.encoder.layers.8.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.8.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.8.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.8.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.8.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.8.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.8.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.8.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.8.layer_norm1.weight, bert_encoder.text_model.encoder.layers.8.layer_norm1.bias, bert_encoder.text_model.encoder.layers.8.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.8.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.8.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.8.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.8.layer_norm2.weight, bert_encoder.text_model.encoder.layers.8.layer_norm2.bias, bert_encoder.text_model.encoder.layers.9.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.9.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.9.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.9.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.9.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.9.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.9.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.9.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.9.layer_norm1.weight, bert_encoder.text_model.encoder.layers.9.layer_norm1.bias, bert_encoder.text_model.encoder.layers.9.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.9.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.9.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.9.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.9.layer_norm2.weight, bert_encoder.text_model.encoder.layers.9.layer_norm2.bias, bert_encoder.text_model.encoder.layers.10.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.10.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.10.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.10.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.10.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.10.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.10.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.10.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.10.layer_norm1.weight, bert_encoder.text_model.encoder.layers.10.layer_norm1.bias, bert_encoder.text_model.encoder.layers.10.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.10.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.10.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.10.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.10.layer_norm2.weight, bert_encoder.text_model.encoder.layers.10.layer_norm2.bias, bert_encoder.text_model.encoder.layers.11.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.11.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.11.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.11.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.11.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.11.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.11.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.11.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.11.layer_norm1.weight, bert_encoder.text_model.encoder.layers.11.layer_norm1.bias, bert_encoder.text_model.encoder.layers.11.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.11.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.11.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.11.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.11.layer_norm2.weight, bert_encoder.text_model.encoder.layers.11.layer_norm2.bias, bert_encoder.text_model.final_layer_norm.weight, bert_encoder.text_model.final_layer_norm.bias, mdin.input_proj.0.weight, mdin.input_proj.0.bias, mdin.input_proj.1.weight, mdin.input_proj.1.bias, mdin.lang_proj.weight, mdin.lang_proj.bias, mdin.lang_norm.weight, mdin.lang_norm.bias, mdin.sampling_module.camap.query_proj.weight, mdin.sampling_module.camap.key_proj.weight, mdin.query_generator.0.weight, mdin.query_generator.0.bias, mdin.query_generator.2.weight, mdin.query_generator.2.bias, mdin.query_generator.4.weight, mdin.query_generator.4.bias, mdin.qsa_layers.0.attn.in_proj_weight, mdin.qsa_layers.0.attn.in_proj_bias, mdin.qsa_layers.0.attn.out_proj.weight, mdin.qsa_layers.0.attn.out_proj.bias, mdin.qsa_layers.0.norm.weight, mdin.qsa_layers.0.norm.bias, mdin.qsa_layers.1.attn.in_proj_weight, mdin.qsa_layers.1.attn.in_proj_bias, mdin.qsa_layers.1.attn.out_proj.weight, mdin.qsa_layers.1.attn.out_proj.bias, mdin.qsa_layers.1.norm.weight, mdin.qsa_layers.1.norm.bias, mdin.qsa_layers.2.attn.in_proj_weight, mdin.qsa_layers.2.attn.in_proj_bias, mdin.qsa_layers.2.attn.out_proj.weight, mdin.qsa_layers.2.attn.out_proj.bias, mdin.qsa_layers.2.norm.weight, mdin.qsa_layers.2.norm.bias, mdin.qsa_layers.3.attn.in_proj_weight, mdin.qsa_layers.3.attn.in_proj_bias, mdin.qsa_layers.3.attn.out_proj.weight, mdin.qsa_layers.3.attn.out_proj.bias, mdin.qsa_layers.3.norm.weight, mdin.qsa_layers.3.norm.bias, mdin.qsa_layers.4.attn.in_proj_weight, mdin.qsa_layers.4.attn.in_proj_bias, mdin.qsa_layers.4.attn.out_proj.weight, mdin.qsa_layers.4.attn.out_proj.bias, mdin.qsa_layers.4.norm.weight, mdin.qsa_layers.4.norm.bias, mdin.qsa_layers.5.attn.in_proj_weight, mdin.qsa_layers.5.attn.in_proj_bias, mdin.qsa_layers.5.attn.out_proj.weight, mdin.qsa_layers.5.attn.out_proj.bias, mdin.qsa_layers.5.norm.weight, mdin.qsa_layers.5.norm.bias, mdin.qqa_layers.0.attn.in_proj_weight, mdin.qqa_layers.0.attn.in_proj_bias, mdin.qqa_layers.0.attn.out_proj.weight, mdin.qqa_layers.0.attn.out_proj.bias, mdin.qqa_layers.0.norm.weight, mdin.qqa_layers.0.norm.bias, mdin.qqa_layers.1.attn.in_proj_weight, mdin.qqa_layers.1.attn.in_proj_bias, mdin.qqa_layers.1.attn.out_proj.weight, mdin.qqa_layers.1.attn.out_proj.bias, mdin.qqa_layers.1.norm.weight, mdin.qqa_layers.1.norm.bias, mdin.qqa_layers.2.attn.in_proj_weight, mdin.qqa_layers.2.attn.in_proj_bias, mdin.qqa_layers.2.attn.out_proj.weight, mdin.qqa_layers.2.attn.out_proj.bias, mdin.qqa_layers.2.norm.weight, mdin.qqa_layers.2.norm.bias, mdin.qqa_layers.3.attn.in_proj_weight, mdin.qqa_layers.3.attn.in_proj_bias, mdin.qqa_layers.3.attn.out_proj.weight, mdin.qqa_layers.3.attn.out_proj.bias, mdin.qqa_layers.3.norm.weight, mdin.qqa_layers.3.norm.bias, mdin.qqa_layers.4.attn.in_proj_weight, mdin.qqa_layers.4.attn.in_proj_bias, mdin.qqa_layers.4.attn.out_proj.weight, mdin.qqa_layers.4.attn.out_proj.bias, mdin.qqa_layers.4.norm.weight, mdin.qqa_layers.4.norm.bias, mdin.qqa_layers.5.attn.in_proj_weight, mdin.qqa_layers.5.attn.in_proj_bias, mdin.qqa_layers.5.attn.out_proj.weight, mdin.qqa_layers.5.attn.out_proj.bias, mdin.qqa_layers.5.norm.weight, mdin.qqa_layers.5.norm.bias, mdin.qla_layers.0.attn.in_proj_weight, mdin.qla_layers.0.attn.in_proj_bias, mdin.qla_layers.0.attn.out_proj.weight, mdin.qla_layers.0.attn.out_proj.bias, mdin.qla_layers.0.norm.weight, mdin.qla_layers.0.norm.bias, mdin.qla_layers.1.attn.in_proj_weight, mdin.qla_layers.1.attn.in_proj_bias, mdin.qla_layers.1.attn.out_proj.weight, mdin.qla_layers.1.attn.out_proj.bias, mdin.qla_layers.1.norm.weight, mdin.qla_layers.1.norm.bias, mdin.qla_layers.2.attn.in_proj_weight, mdin.qla_layers.2.attn.in_proj_bias, mdin.qla_layers.2.attn.out_proj.weight, mdin.qla_layers.2.attn.out_proj.bias, mdin.qla_layers.2.norm.weight, mdin.qla_layers.2.norm.bias, mdin.qla_layers.3.attn.in_proj_weight, mdin.qla_layers.3.attn.in_proj_bias, mdin.qla_layers.3.attn.out_proj.weight, mdin.qla_layers.3.attn.out_proj.bias, mdin.qla_layers.3.norm.weight, mdin.qla_layers.3.norm.bias, mdin.qla_layers.4.attn.in_proj_weight, mdin.qla_layers.4.attn.in_proj_bias, mdin.qla_layers.4.attn.out_proj.weight, mdin.qla_layers.4.attn.out_proj.bias, mdin.qla_layers.4.norm.weight, mdin.qla_layers.4.norm.bias, mdin.qla_layers.5.attn.in_proj_weight, mdin.qla_layers.5.attn.in_proj_bias, mdin.qla_layers.5.attn.out_proj.weight, mdin.qla_layers.5.attn.out_proj.bias, mdin.qla_layers.5.norm.weight, mdin.qla_layers.5.norm.bias, mdin.qla_ffn_layers.0.net.0.weight, mdin.qla_ffn_layers.0.net.0.bias, mdin.qla_ffn_layers.0.net.3.weight, mdin.qla_ffn_layers.0.net.3.bias, mdin.qla_ffn_layers.0.norm.weight, mdin.qla_ffn_layers.0.norm.bias, mdin.qla_ffn_layers.1.net.0.weight, mdin.qla_ffn_layers.1.net.0.bias, mdin.qla_ffn_layers.1.net.3.weight, mdin.qla_ffn_layers.1.net.3.bias, mdin.qla_ffn_layers.1.norm.weight, mdin.qla_ffn_layers.1.norm.bias, mdin.qla_ffn_layers.2.net.0.weight, mdin.qla_ffn_layers.2.net.0.bias, mdin.qla_ffn_layers.2.net.3.weight, mdin.qla_ffn_layers.2.net.3.bias, mdin.qla_ffn_layers.2.norm.weight, mdin.qla_ffn_layers.2.norm.bias, mdin.qla_ffn_layers.3.net.0.weight, mdin.qla_ffn_layers.3.net.0.bias, mdin.qla_ffn_layers.3.net.3.weight, mdin.qla_ffn_layers.3.net.3.bias, mdin.qla_ffn_layers.3.norm.weight, mdin.qla_ffn_layers.3.norm.bias, mdin.qla_ffn_layers.4.net.0.weight, mdin.qla_ffn_layers.4.net.0.bias, mdin.qla_ffn_layers.4.net.3.weight, mdin.qla_ffn_layers.4.net.3.bias, mdin.qla_ffn_layers.4.norm.weight, mdin.qla_ffn_layers.4.norm.bias, mdin.qla_ffn_layers.5.net.0.weight, mdin.qla_ffn_layers.5.net.0.bias, mdin.qla_ffn_layers.5.net.3.weight, mdin.qla_ffn_layers.5.net.3.bias, mdin.qla_ffn_layers.5.norm.weight, mdin.qla_ffn_layers.5.norm.bias, mdin.lla_layers.0.attn.in_proj_weight, mdin.lla_layers.0.attn.in_proj_bias, mdin.lla_layers.0.attn.out_proj.weight, mdin.lla_layers.0.attn.out_proj.bias, mdin.lla_layers.0.norm.weight, mdin.lla_layers.0.norm.bias, mdin.lla_layers.1.attn.in_proj_weight, mdin.lla_layers.1.attn.in_proj_bias, mdin.lla_layers.1.attn.out_proj.weight, mdin.lla_layers.1.attn.out_proj.bias, mdin.lla_layers.1.norm.weight, mdin.lla_layers.1.norm.bias, mdin.lla_layers.2.attn.in_proj_weight, mdin.lla_layers.2.attn.in_proj_bias, mdin.lla_layers.2.attn.out_proj.weight, mdin.lla_layers.2.attn.out_proj.bias, mdin.lla_layers.2.norm.weight, mdin.lla_layers.2.norm.bias, mdin.lla_layers.3.attn.in_proj_weight, mdin.lla_layers.3.attn.in_proj_bias, mdin.lla_layers.3.attn.out_proj.weight, mdin.lla_layers.3.attn.out_proj.bias, mdin.lla_layers.3.norm.weight, mdin.lla_layers.3.norm.bias, mdin.lla_layers.4.attn.in_proj_weight, mdin.lla_layers.4.attn.in_proj_bias, mdin.lla_layers.4.attn.out_proj.weight, mdin.lla_layers.4.attn.out_proj.bias, mdin.lla_layers.4.norm.weight, mdin.lla_layers.4.norm.bias, mdin.lla_layers.5.attn.in_proj_weight, mdin.lla_layers.5.attn.in_proj_bias, mdin.lla_layers.5.attn.out_proj.weight, mdin.lla_layers.5.attn.out_proj.bias, mdin.lla_layers.5.norm.weight, mdin.lla_layers.5.norm.bias, mdin.lsa_layers.0.attn.in_proj_weight, mdin.lsa_layers.0.attn.in_proj_bias, mdin.lsa_layers.0.attn.out_proj.weight, mdin.lsa_layers.0.attn.out_proj.bias, mdin.lsa_layers.0.norm.weight, mdin.lsa_layers.0.norm.bias, mdin.lsa_layers.1.attn.in_proj_weight, mdin.lsa_layers.1.attn.in_proj_bias, mdin.lsa_layers.1.attn.out_proj.weight, mdin.lsa_layers.1.attn.out_proj.bias, mdin.lsa_layers.1.norm.weight, mdin.lsa_layers.1.norm.bias, mdin.lsa_layers.2.attn.in_proj_weight, mdin.lsa_layers.2.attn.in_proj_bias, mdin.lsa_layers.2.attn.out_proj.weight, mdin.lsa_layers.2.attn.out_proj.bias, mdin.lsa_layers.2.norm.weight, mdin.lsa_layers.2.norm.bias, mdin.lsa_layers.3.attn.in_proj_weight, mdin.lsa_layers.3.attn.in_proj_bias, mdin.lsa_layers.3.attn.out_proj.weight, mdin.lsa_layers.3.attn.out_proj.bias, mdin.lsa_layers.3.norm.weight, mdin.lsa_layers.3.norm.bias, mdin.lsa_layers.4.attn.in_proj_weight, mdin.lsa_layers.4.attn.in_proj_bias, mdin.lsa_layers.4.attn.out_proj.weight, mdin.lsa_layers.4.attn.out_proj.bias, mdin.lsa_layers.4.norm.weight, mdin.lsa_layers.4.norm.bias, mdin.lsa_layers.5.attn.in_proj_weight, mdin.lsa_layers.5.attn.in_proj_bias, mdin.lsa_layers.5.attn.out_proj.weight, mdin.lsa_layers.5.attn.out_proj.bias, mdin.lsa_layers.5.norm.weight, mdin.lsa_layers.5.norm.bias, mdin.lsa_ffn_layers.0.net.0.weight, mdin.lsa_ffn_layers.0.net.0.bias, mdin.lsa_ffn_layers.0.net.3.weight, mdin.lsa_ffn_layers.0.net.3.bias, mdin.lsa_ffn_layers.0.norm.weight, mdin.lsa_ffn_layers.0.norm.bias, mdin.lsa_ffn_layers.1.net.0.weight, mdin.lsa_ffn_layers.1.net.0.bias, mdin.lsa_ffn_layers.1.net.3.weight, mdin.lsa_ffn_layers.1.net.3.bias, mdin.lsa_ffn_layers.1.norm.weight, mdin.lsa_ffn_layers.1.norm.bias, mdin.lsa_ffn_layers.2.net.0.weight, mdin.lsa_ffn_layers.2.net.0.bias, mdin.lsa_ffn_layers.2.net.3.weight, mdin.lsa_ffn_layers.2.net.3.bias, mdin.lsa_ffn_layers.2.norm.weight, mdin.lsa_ffn_layers.2.norm.bias, mdin.lsa_ffn_layers.3.net.0.weight, mdin.lsa_ffn_layers.3.net.0.bias, mdin.lsa_ffn_layers.3.net.3.weight, mdin.lsa_ffn_layers.3.net.3.bias, mdin.lsa_ffn_layers.3.norm.weight, mdin.lsa_ffn_layers.3.norm.bias, mdin.lsa_ffn_layers.4.net.0.weight, mdin.lsa_ffn_layers.4.net.0.bias, mdin.lsa_ffn_layers.4.net.3.weight, mdin.lsa_ffn_layers.4.net.3.bias, mdin.lsa_ffn_layers.4.norm.weight, mdin.lsa_ffn_layers.4.norm.bias, mdin.lsa_ffn_layers.5.net.0.weight, mdin.lsa_ffn_layers.5.net.0.bias, mdin.lsa_ffn_layers.5.net.3.weight, mdin.lsa_ffn_layers.5.net.3.bias, mdin.lsa_ffn_layers.5.norm.weight, mdin.lsa_ffn_layers.5.norm.bias, mdin.out_norm.weight, mdin.out_norm.bias, mdin.out_score.0.weight, mdin.out_score.0.bias, mdin.out_score.2.weight, mdin.out_score.2.bias, mdin.x_mask.0.weight, mdin.x_mask.0.bias, mdin.x_mask.2.weight, mdin.x_mask.2.bias, mdin.indi_embedding.0.weight, mdin.indi_embedding.0.bias, mdin.indi_embedding.2.weight, mdin.indi_embedding.2.bias, mdin.indi_embedding.3.weight, mdin.indi_embedding.3.bias, mdin.indi_norm.weight, mdin.indi_norm.bias, mdin.contrastive_align_projection_vision.0.weight, mdin.contrastive_align_projection_vision.0.bias, mdin.contrastive_align_projection_vision.2.weight, mdin.contrastive_align_projection_vision.2.bias, mdin.contrastive_align_projection_vision.4.weight, mdin.contrastive_align_projection_vision.4.bias, mdin.contrastive_align_projection_text.0.weight, mdin.contrastive_align_projection_text.0.bias, mdin.contrastive_align_projection_text.2.weight, mdin.contrastive_align_projection_text.2.bias, mdin.contrastive_align_projection_text.4.weight, mdin.contrastive_align_projection_text.4.bias, criterion.matcher.cost_weight

2025-04-26 16:12:39,148 - INFO - Train batch size per gpu: 7
2025-04-26 16:12:39,361 - INFO - Load train multi3drefer: 43838 samples
2025-04-26 16:12:43,432 - INFO - Load val multi3drefer: 11120 samples
2025-04-26 16:12:44,664 - INFO - Training
tools/train_3dgres.py:85: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
2025-04-26 16:13:48,998 - INFO - Epoch [1/70][10/817]  lr: 0.0001, eta: 4 days, 6:10:04, data_time: 1.58, iter_time: 6.43, score_loss: 0.0000, mask_bce_loss: 0.0406, mask_dice_loss: 0.1181, sem_loss: 5.4135, indi_loss: 1.8182, sample_loss: 0.1456, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0244, layer_0_mask_dice_loss: 0.1228, layer_0_sem_loss: 5.4645, layer_0_indi_loss: 2.2838, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0284, layer_1_mask_dice_loss: 0.1238, layer_1_sem_loss: 5.4525, layer_1_indi_loss: 1.7839, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0241, layer_2_mask_dice_loss: 0.1209, layer_2_sem_loss: 5.4645, layer_2_indi_loss: 1.8282, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0223, layer_3_mask_dice_loss: 0.1197, layer_3_sem_loss: 5.5127, layer_3_indi_loss: 1.8806, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0512, layer_4_mask_dice_loss: 0.1172, layer_4_sem_loss: 5.4266, layer_4_indi_loss: 1.8961, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0438, layer_5_mask_dice_loss: 0.1184, layer_5_sem_loss: 5.4272, layer_5_indi_loss: 1.8336, loss: 4.2748, grad_total_norm: 10.3300
2025-04-26 16:14:34,360 - INFO - Epoch [1/70][20/817]  lr: 0.0001, eta: 3 days, 15:05:39, data_time: 0.79, iter_time: 5.48, score_loss: 0.0000, mask_bce_loss: 0.0227, mask_dice_loss: 0.1418, sem_loss: 4.8946, indi_loss: 2.1080, sample_loss: 0.1441, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0213, layer_0_mask_dice_loss: 0.1402, layer_0_sem_loss: 5.1151, layer_0_indi_loss: 2.6618, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0258, layer_1_mask_dice_loss: 0.1410, layer_1_sem_loss: 4.8391, layer_1_indi_loss: 1.9782, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0381, layer_2_mask_dice_loss: 0.1380, layer_2_sem_loss: 4.8736, layer_2_indi_loss: 1.9551, layer_3_score_loss: 0.0008, layer_3_mask_bce_loss: 0.0311, layer_3_mask_dice_loss: 0.1374, layer_3_sem_loss: 4.8445, layer_3_indi_loss: 1.9693, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0230, layer_4_mask_dice_loss: 0.1402, layer_4_sem_loss: 4.8736, layer_4_indi_loss: 1.9922, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0224, layer_5_mask_dice_loss: 0.1417, layer_5_sem_loss: 4.9149, layer_5_indi_loss: 2.1198, loss: 4.1665, grad_total_norm: 9.9472
2025-04-26 16:15:18,732 - INFO - Epoch [1/70][30/817]  lr: 0.0001, eta: 3 days, 9:32:11, data_time: 0.53, iter_time: 5.14, score_loss: 0.0000, mask_bce_loss: 0.0093, mask_dice_loss: 0.0530, sem_loss: 2.5590, indi_loss: 1.5045, sample_loss: 0.1387, layer_0_score_loss: 0.0013, layer_0_mask_bce_loss: 0.0100, layer_0_mask_dice_loss: 0.0524, layer_0_sem_loss: 2.5271, layer_0_indi_loss: 1.2259, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0103, layer_1_mask_dice_loss: 0.0537, layer_1_sem_loss: 2.5539, layer_1_indi_loss: 1.3600, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0097, layer_2_mask_dice_loss: 0.0535, layer_2_sem_loss: 2.5789, layer_2_indi_loss: 1.4274, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0099, layer_3_mask_dice_loss: 0.0533, layer_3_sem_loss: 2.5766, layer_3_indi_loss: 1.4346, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0098, layer_4_mask_dice_loss: 0.0534, layer_4_sem_loss: 2.5562, layer_4_indi_loss: 1.4733, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0092, layer_5_mask_dice_loss: 0.0528, layer_5_sem_loss: 2.5607, layer_5_indi_loss: 1.4764, loss: 2.5514, grad_total_norm: 9.4626
2025-04-26 16:15:59,194 - INFO - Epoch [1/70][40/817]  lr: 0.0001, eta: 3 days, 5:12:02, data_time: 0.40, iter_time: 4.86, score_loss: 0.0000, mask_bce_loss: 0.0157, mask_dice_loss: 0.0884, sem_loss: 2.9078, indi_loss: 1.6427, sample_loss: 0.1448, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0161, layer_0_mask_dice_loss: 0.0877, layer_0_sem_loss: 3.0011, layer_0_indi_loss: 1.8798, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0201, layer_1_mask_dice_loss: 0.0905, layer_1_sem_loss: 2.9405, layer_1_indi_loss: 1.6162, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0187, layer_2_mask_dice_loss: 0.0899, layer_2_sem_loss: 2.9292, layer_2_indi_loss: 1.6369, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0186, layer_3_mask_dice_loss: 0.0894, layer_3_sem_loss: 2.9245, layer_3_indi_loss: 1.6300, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0173, layer_4_mask_dice_loss: 0.0897, layer_4_sem_loss: 2.9171, layer_4_indi_loss: 1.6403, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0169, layer_5_mask_dice_loss: 0.0888, layer_5_sem_loss: 2.9101, layer_5_indi_loss: 1.6513, loss: 2.9773, grad_total_norm: 3.7728
2025-04-26 16:16:40,414 - INFO - Epoch [1/70][50/817]  lr: 0.0001, eta: 3 days, 2:50:02, data_time: 0.32, iter_time: 4.71, score_loss: 0.0000, mask_bce_loss: 0.0144, mask_dice_loss: 0.1133, sem_loss: 2.9814, indi_loss: 1.7136, sample_loss: 0.1448, layer_0_score_loss: 0.0001, layer_0_mask_bce_loss: 0.0156, layer_0_mask_dice_loss: 0.1096, layer_0_sem_loss: 3.0248, layer_0_indi_loss: 2.0645, layer_1_score_loss: 0.0004, layer_1_mask_bce_loss: 0.0159, layer_1_mask_dice_loss: 0.1139, layer_1_sem_loss: 2.9940, layer_1_indi_loss: 1.7149, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0175, layer_2_mask_dice_loss: 0.1137, layer_2_sem_loss: 3.0031, layer_2_indi_loss: 1.7095, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0202, layer_3_mask_dice_loss: 0.1131, layer_3_sem_loss: 3.0067, layer_3_indi_loss: 1.7092, layer_4_score_loss: 0.0001, layer_4_mask_bce_loss: 0.0169, layer_4_mask_dice_loss: 0.1129, layer_4_sem_loss: 3.0106, layer_4_indi_loss: 1.7037, layer_5_score_loss: 0.0001, layer_5_mask_bce_loss: 0.0161, layer_5_mask_dice_loss: 0.1139, layer_5_sem_loss: 2.9874, layer_5_indi_loss: 1.7052, loss: 3.1309, grad_total_norm: 7.0129
2025-04-26 16:17:24,317 - INFO - Epoch [1/70][60/817]  lr: 0.0001, eta: 3 days, 1:57:47, data_time: 0.27, iter_time: 4.66, score_loss: 0.0000, mask_bce_loss: 0.0153, mask_dice_loss: 0.1501, sem_loss: 4.8542, indi_loss: 2.1223, sample_loss: 0.1464, layer_0_score_loss: 0.0002, layer_0_mask_bce_loss: 0.0144, layer_0_mask_dice_loss: 0.1417, layer_0_sem_loss: 4.8259, layer_0_indi_loss: 2.8802, layer_1_score_loss: 0.0001, layer_1_mask_bce_loss: 0.0152, layer_1_mask_dice_loss: 0.1505, layer_1_sem_loss: 4.8250, layer_1_indi_loss: 2.1106, layer_2_score_loss: 0.0001, layer_2_mask_bce_loss: 0.0146, layer_2_mask_dice_loss: 0.1492, layer_2_sem_loss: 4.8589, layer_2_indi_loss: 2.0985, layer_3_score_loss: 0.0001, layer_3_mask_bce_loss: 0.0146, layer_3_mask_dice_loss: 0.1485, layer_3_sem_loss: 4.8237, layer_3_indi_loss: 2.1126, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0145, layer_4_mask_dice_loss: 0.1505, layer_4_sem_loss: 4.8211, layer_4_indi_loss: 2.1127, layer_5_score_loss: 0.0004, layer_5_mask_bce_loss: 0.0155, layer_5_mask_dice_loss: 0.1487, layer_5_sem_loss: 4.7926, layer_5_indi_loss: 2.1093, loss: 4.1772, grad_total_norm: 10.1617
2025-04-26 16:18:04,781 - INFO - Epoch [1/70][70/817]  lr: 0.0001, eta: 3 days, 0:33:29, data_time: 0.23, iter_time: 4.57, score_loss: 0.0000, mask_bce_loss: 0.0195, mask_dice_loss: 0.1276, sem_loss: 3.0483, indi_loss: 1.8578, sample_loss: 0.1446, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0186, layer_0_mask_dice_loss: 0.1195, layer_0_sem_loss: 3.2344, layer_0_indi_loss: 2.3877, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0200, layer_1_mask_dice_loss: 0.1259, layer_1_sem_loss: 3.0844, layer_1_indi_loss: 1.8669, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0192, layer_2_mask_dice_loss: 0.1260, layer_2_sem_loss: 3.0688, layer_2_indi_loss: 1.8527, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0187, layer_3_mask_dice_loss: 0.1256, layer_3_sem_loss: 3.0517, layer_3_indi_loss: 1.8501, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0188, layer_4_mask_dice_loss: 0.1265, layer_4_sem_loss: 3.0523, layer_4_indi_loss: 1.8549, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0196, layer_5_mask_dice_loss: 0.1272, layer_5_sem_loss: 3.0476, layer_5_indi_loss: 1.8512, loss: 3.2795, grad_total_norm: 9.4949
2025-04-26 16:18:47,616 - INFO - Epoch [1/70][80/817]  lr: 0.0001, eta: 2 days, 23:58:17, data_time: 0.20, iter_time: 4.54, score_loss: 0.0000, mask_bce_loss: 0.0215, mask_dice_loss: 0.1762, sem_loss: 4.3412, indi_loss: 2.0832, sample_loss: 0.1455, layer_0_score_loss: 0.0002, layer_0_mask_bce_loss: 0.0153, layer_0_mask_dice_loss: 0.1638, layer_0_sem_loss: 4.4287, layer_0_indi_loss: 2.9853, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0198, layer_1_mask_dice_loss: 0.1738, layer_1_sem_loss: 4.3702, layer_1_indi_loss: 2.1121, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0190, layer_2_mask_dice_loss: 0.1736, layer_2_sem_loss: 4.3893, layer_2_indi_loss: 2.0952, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0198, layer_3_mask_dice_loss: 0.1740, layer_3_sem_loss: 4.3777, layer_3_indi_loss: 2.0921, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0199, layer_4_mask_dice_loss: 0.1753, layer_4_sem_loss: 4.3629, layer_4_indi_loss: 2.0883, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0207, layer_5_mask_dice_loss: 0.1764, layer_5_sem_loss: 4.3308, layer_5_indi_loss: 2.0832, loss: 4.0996, grad_total_norm: 13.3936
2025-04-26 16:19:31,004 - INFO - Epoch [1/70][90/817]  lr: 0.0001, eta: 2 days, 23:36:35, data_time: 0.18, iter_time: 4.51, score_loss: 0.0000, mask_bce_loss: 0.0220, mask_dice_loss: 0.1237, sem_loss: 4.0919, indi_loss: 1.9361, sample_loss: 0.1456, layer_0_score_loss: 0.0003, layer_0_mask_bce_loss: 0.0189, layer_0_mask_dice_loss: 0.1152, layer_0_sem_loss: 4.0756, layer_0_indi_loss: 2.4681, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0234, layer_1_mask_dice_loss: 0.1212, layer_1_sem_loss: 4.0730, layer_1_indi_loss: 1.9406, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0224, layer_2_mask_dice_loss: 0.1209, layer_2_sem_loss: 4.0870, layer_2_indi_loss: 1.9332, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0220, layer_3_mask_dice_loss: 0.1215, layer_3_sem_loss: 4.0791, layer_3_indi_loss: 1.9333, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0226, layer_4_mask_dice_loss: 0.1228, layer_4_sem_loss: 4.0858, layer_4_indi_loss: 1.9325, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0229, layer_5_mask_dice_loss: 0.1239, layer_5_sem_loss: 4.0777, layer_5_indi_loss: 1.9314, loss: 3.7218, grad_total_norm: 9.6391
2025-04-26 16:20:13,902 - INFO - Epoch [1/70][100/817]  lr: 0.0001, eta: 2 days, 23:14:25, data_time: 0.16, iter_time: 4.49, score_loss: 0.0000, mask_bce_loss: 0.0130, mask_dice_loss: 0.0810, sem_loss: 1.8542, indi_loss: 1.5341, sample_loss: 0.1399, layer_0_score_loss: 0.0006, layer_0_mask_bce_loss: 0.0105, layer_0_mask_dice_loss: 0.0704, layer_0_sem_loss: 1.9967, layer_0_indi_loss: 1.6786, layer_1_score_loss: 0.0004, layer_1_mask_bce_loss: 0.0124, layer_1_mask_dice_loss: 0.0767, layer_1_sem_loss: 1.8606, layer_1_indi_loss: 1.5101, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0131, layer_2_mask_dice_loss: 0.0782, layer_2_sem_loss: 1.8612, layer_2_indi_loss: 1.5124, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0129, layer_3_mask_dice_loss: 0.0789, layer_3_sem_loss: 1.8557, layer_3_indi_loss: 1.5138, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0130, layer_4_mask_dice_loss: 0.0808, layer_4_sem_loss: 1.8508, layer_4_indi_loss: 1.5239, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0123, layer_5_mask_dice_loss: 0.0805, layer_5_sem_loss: 1.8565, layer_5_indi_loss: 1.5282, loss: 2.4247, grad_total_norm: 5.2009
2025-04-26 16:20:55,508 - INFO - Epoch [1/70][110/817]  lr: 0.0001, eta: 2 days, 22:44:59, data_time: 0.15, iter_time: 4.46, score_loss: 0.0000, mask_bce_loss: 0.0129, mask_dice_loss: 0.0752, sem_loss: 2.4040, indi_loss: 1.4560, sample_loss: 0.1436, layer_0_score_loss: 0.0001, layer_0_mask_bce_loss: 0.0097, layer_0_mask_dice_loss: 0.0674, layer_0_sem_loss: 2.4475, layer_0_indi_loss: 1.5998, layer_1_score_loss: 0.0002, layer_1_mask_bce_loss: 0.0118, layer_1_mask_dice_loss: 0.0725, layer_1_sem_loss: 2.3891, layer_1_indi_loss: 1.4714, layer_2_score_loss: 0.0002, layer_2_mask_bce_loss: 0.0119, layer_2_mask_dice_loss: 0.0728, layer_2_sem_loss: 2.4004, layer_2_indi_loss: 1.4668, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0111, layer_3_mask_dice_loss: 0.0719, layer_3_sem_loss: 2.3944, layer_3_indi_loss: 1.4576, layer_4_score_loss: 0.0001, layer_4_mask_bce_loss: 0.0117, layer_4_mask_dice_loss: 0.0730, layer_4_sem_loss: 2.3998, layer_4_indi_loss: 1.4580, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0125, layer_5_mask_dice_loss: 0.0745, layer_5_sem_loss: 2.3972, layer_5_indi_loss: 1.4627, loss: 2.6066, grad_total_norm: 9.6621
2025-04-26 16:21:38,842 - INFO - Epoch [1/70][120/817]  lr: 0.0001, eta: 2 days, 22:34:02, data_time: 0.13, iter_time: 4.45, score_loss: 0.0000, mask_bce_loss: 0.0243, mask_dice_loss: 0.1308, sem_loss: 3.3282, indi_loss: 1.8473, sample_loss: 0.1466, layer_0_score_loss: 0.0003, layer_0_mask_bce_loss: 0.0169, layer_0_mask_dice_loss: 0.1187, layer_0_sem_loss: 3.2791, layer_0_indi_loss: 2.4146, layer_1_score_loss: 0.0001, layer_1_mask_bce_loss: 0.0189, layer_1_mask_dice_loss: 0.1257, layer_1_sem_loss: 3.2399, layer_1_indi_loss: 1.8578, layer_2_score_loss: 0.0001, layer_2_mask_bce_loss: 0.0200, layer_2_mask_dice_loss: 0.1251, layer_2_sem_loss: 3.2419, layer_2_indi_loss: 1.8474, layer_3_score_loss: 0.0001, layer_3_mask_bce_loss: 0.0214, layer_3_mask_dice_loss: 0.1257, layer_3_sem_loss: 3.2801, layer_3_indi_loss: 1.8391, layer_4_score_loss: 0.0002, layer_4_mask_bce_loss: 0.0220, layer_4_mask_dice_loss: 0.1276, layer_4_sem_loss: 3.3068, layer_4_indi_loss: 1.8412, layer_5_score_loss: 0.0001, layer_5_mask_bce_loss: 0.0231, layer_5_mask_dice_loss: 0.1289, layer_5_sem_loss: 3.2847, layer_5_indi_loss: 1.8447, loss: 3.3953, grad_total_norm: 13.2621
2025-04-26 16:22:21,783 - INFO - Epoch [1/70][130/817]  lr: 0.0001, eta: 2 days, 22:21:46, data_time: 0.12, iter_time: 4.44, score_loss: 0.0000, mask_bce_loss: 0.0108, mask_dice_loss: 0.0897, sem_loss: 1.9668, indi_loss: 1.5156, sample_loss: 0.1426, layer_0_score_loss: 0.0001, layer_0_mask_bce_loss: 0.0103, layer_0_mask_dice_loss: 0.0823, layer_0_sem_loss: 2.0211, layer_0_indi_loss: 1.7203, layer_1_score_loss: 0.0004, layer_1_mask_bce_loss: 0.0106, layer_1_mask_dice_loss: 0.0868, layer_1_sem_loss: 1.9737, layer_1_indi_loss: 1.5383, layer_2_score_loss: 0.0002, layer_2_mask_bce_loss: 0.0110, layer_2_mask_dice_loss: 0.0872, layer_2_sem_loss: 1.9625, layer_2_indi_loss: 1.5285, layer_3_score_loss: 0.0003, layer_3_mask_bce_loss: 0.0108, layer_3_mask_dice_loss: 0.0867, layer_3_sem_loss: 1.9560, layer_3_indi_loss: 1.5130, layer_4_score_loss: 0.0001, layer_4_mask_bce_loss: 0.0111, layer_4_mask_dice_loss: 0.0871, layer_4_sem_loss: 1.9581, layer_4_indi_loss: 1.5129, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0109, layer_5_mask_dice_loss: 0.0887, layer_5_sem_loss: 1.9612, layer_5_indi_loss: 1.5111, loss: 2.5036, grad_total_norm: 2.9789
2025-04-26 16:23:02,345 - INFO - Epoch [1/70][140/817]  lr: 0.0001, eta: 2 days, 21:55:01, data_time: 0.11, iter_time: 4.41, score_loss: 0.0001, mask_bce_loss: 0.0111, mask_dice_loss: 0.0800, sem_loss: 1.8994, indi_loss: 1.4784, sample_loss: 0.1446, layer_0_score_loss: 0.0005, layer_0_mask_bce_loss: 0.0093, layer_0_mask_dice_loss: 0.0738, layer_0_sem_loss: 1.9540, layer_0_indi_loss: 1.6889, layer_1_score_loss: 0.0003, layer_1_mask_bce_loss: 0.0106, layer_1_mask_dice_loss: 0.0774, layer_1_sem_loss: 1.9172, layer_1_indi_loss: 1.4856, layer_2_score_loss: 0.0001, layer_2_mask_bce_loss: 0.0110, layer_2_mask_dice_loss: 0.0766, layer_2_sem_loss: 1.9124, layer_2_indi_loss: 1.4603, layer_3_score_loss: 0.0003, layer_3_mask_bce_loss: 0.0110, layer_3_mask_dice_loss: 0.0766, layer_3_sem_loss: 1.9025, layer_3_indi_loss: 1.4568, layer_4_score_loss: 0.0003, layer_4_mask_bce_loss: 0.0118, layer_4_mask_dice_loss: 0.0767, layer_4_sem_loss: 1.9101, layer_4_indi_loss: 1.4670, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0135, layer_5_mask_dice_loss: 0.0784, layer_5_sem_loss: 1.9021, layer_5_indi_loss: 1.4624, loss: 2.4346, grad_total_norm: 5.0418
2025-04-26 16:23:44,559 - INFO - Epoch [1/70][150/817]  lr: 0.0001, eta: 2 days, 21:42:13, data_time: 0.11, iter_time: 4.40, score_loss: 0.0001, mask_bce_loss: 0.0168, mask_dice_loss: 0.0771, sem_loss: 1.5995, indi_loss: 1.3895, sample_loss: 0.1427, layer_0_score_loss: 0.0003, layer_0_mask_bce_loss: 0.0101, layer_0_mask_dice_loss: 0.0689, layer_0_sem_loss: 1.6284, layer_0_indi_loss: 1.6363, layer_1_score_loss: 0.0002, layer_1_mask_bce_loss: 0.0123, layer_1_mask_dice_loss: 0.0741, layer_1_sem_loss: 1.6109, layer_1_indi_loss: 1.4248, layer_2_score_loss: 0.0003, layer_2_mask_bce_loss: 0.0137, layer_2_mask_dice_loss: 0.0728, layer_2_sem_loss: 1.6101, layer_2_indi_loss: 1.3839, layer_3_score_loss: 0.0003, layer_3_mask_bce_loss: 0.0142, layer_3_mask_dice_loss: 0.0730, layer_3_sem_loss: 1.6040, layer_3_indi_loss: 1.3759, layer_4_score_loss: 0.0002, layer_4_mask_bce_loss: 0.0145, layer_4_mask_dice_loss: 0.0734, layer_4_sem_loss: 1.6084, layer_4_indi_loss: 1.3716, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0151, layer_5_mask_dice_loss: 0.0758, layer_5_sem_loss: 1.6138, layer_5_indi_loss: 1.3744, loss: 2.2714, grad_total_norm: 4.0510
2025-04-26 16:24:26,950 - INFO - Epoch [1/70][160/817]  lr: 0.0001, eta: 2 days, 21:31:58, data_time: 0.10, iter_time: 4.39, score_loss: 0.0001, mask_bce_loss: 0.0140, mask_dice_loss: 0.0979, sem_loss: 2.7552, indi_loss: 1.7611, sample_loss: 0.1449, layer_0_score_loss: 0.0007, layer_0_mask_bce_loss: 0.0114, layer_0_mask_dice_loss: 0.0941, layer_0_sem_loss: 2.7774, layer_0_indi_loss: 1.9571, layer_1_score_loss: 0.0006, layer_1_mask_bce_loss: 0.0128, layer_1_mask_dice_loss: 0.0973, layer_1_sem_loss: 2.7602, layer_1_indi_loss: 1.7377, layer_2_score_loss: 0.0003, layer_2_mask_bce_loss: 0.0125, layer_2_mask_dice_loss: 0.0948, layer_2_sem_loss: 2.7617, layer_2_indi_loss: 1.7505, layer_3_score_loss: 0.0005, layer_3_mask_bce_loss: 0.0122, layer_3_mask_dice_loss: 0.0926, layer_3_sem_loss: 2.7635, layer_3_indi_loss: 1.7569, layer_4_score_loss: 0.0004, layer_4_mask_bce_loss: 0.0123, layer_4_mask_dice_loss: 0.0932, layer_4_sem_loss: 2.7604, layer_4_indi_loss: 1.7659, layer_5_score_loss: 0.0004, layer_5_mask_bce_loss: 0.0139, layer_5_mask_dice_loss: 0.0955, layer_5_sem_loss: 2.7581, layer_5_indi_loss: 1.7683, loss: 2.9702, grad_total_norm: 3.4904
2025-04-26 16:25:08,795 - INFO - Epoch [1/70][170/817]  lr: 0.0001, eta: 2 days, 21:19:47, data_time: 0.09, iter_time: 4.38, score_loss: 0.0004, mask_bce_loss: 0.0135, mask_dice_loss: 0.0962, sem_loss: 2.5676, indi_loss: 1.8081, sample_loss: 0.1441, layer_0_score_loss: 0.0006, layer_0_mask_bce_loss: 0.0133, layer_0_mask_dice_loss: 0.0917, layer_0_sem_loss: 2.5914, layer_0_indi_loss: 2.1086, layer_1_score_loss: 0.0005, layer_1_mask_bce_loss: 0.0127, layer_1_mask_dice_loss: 0.0956, layer_1_sem_loss: 2.6023, layer_1_indi_loss: 1.8247, layer_2_score_loss: 0.0006, layer_2_mask_bce_loss: 0.0124, layer_2_mask_dice_loss: 0.0932, layer_2_sem_loss: 2.5876, layer_2_indi_loss: 1.8292, layer_3_score_loss: 0.0004, layer_3_mask_bce_loss: 0.0143, layer_3_mask_dice_loss: 0.0924, layer_3_sem_loss: 2.5816, layer_3_indi_loss: 1.8172, layer_4_score_loss: 0.0004, layer_4_mask_bce_loss: 0.0135, layer_4_mask_dice_loss: 0.0939, layer_4_sem_loss: 2.5710, layer_4_indi_loss: 1.8186, layer_5_score_loss: 0.0007, layer_5_mask_bce_loss: 0.0130, layer_5_mask_dice_loss: 0.0963, layer_5_sem_loss: 2.5775, layer_5_indi_loss: 1.8206, loss: 2.9170, grad_total_norm: 4.6764
2025-04-26 16:25:50,422 - INFO - Epoch [1/70][180/817]  lr: 0.0001, eta: 2 days, 21:07:44, data_time: 0.09, iter_time: 4.37, score_loss: 0.0004, mask_bce_loss: 0.0153, mask_dice_loss: 0.0750, sem_loss: 2.2653, indi_loss: 1.5590, sample_loss: 0.1425, layer_0_score_loss: 0.0006, layer_0_mask_bce_loss: 0.0124, layer_0_mask_dice_loss: 0.0693, layer_0_sem_loss: 2.2585, layer_0_indi_loss: 1.7423, layer_1_score_loss: 0.0005, layer_1_mask_bce_loss: 0.0147, layer_1_mask_dice_loss: 0.0721, layer_1_sem_loss: 2.2501, layer_1_indi_loss: 1.5729, layer_2_score_loss: 0.0009, layer_2_mask_bce_loss: 0.0163, layer_2_mask_dice_loss: 0.0696, layer_2_sem_loss: 2.2501, layer_2_indi_loss: 1.5633, layer_3_score_loss: 0.0009, layer_3_mask_bce_loss: 0.0147, layer_3_mask_dice_loss: 0.0690, layer_3_sem_loss: 2.2527, layer_3_indi_loss: 1.5429, layer_4_score_loss: 0.0006, layer_4_mask_bce_loss: 0.0150, layer_4_mask_dice_loss: 0.0684, layer_4_sem_loss: 2.2575, layer_4_indi_loss: 1.5381, layer_5_score_loss: 0.0004, layer_5_mask_bce_loss: 0.0160, layer_5_mask_dice_loss: 0.0708, layer_5_sem_loss: 2.2605, layer_5_indi_loss: 1.5564, loss: 2.5877, grad_total_norm: 10.8966
2025-04-26 16:26:35,365 - INFO - Epoch [1/70][190/817]  lr: 0.0001, eta: 2 days, 21:13:28, data_time: 0.08, iter_time: 4.37, score_loss: 0.0009, mask_bce_loss: 0.0103, mask_dice_loss: 0.0787, sem_loss: 2.2876, indi_loss: 1.6694, sample_loss: 0.1415, layer_0_score_loss: 0.0006, layer_0_mask_bce_loss: 0.0091, layer_0_mask_dice_loss: 0.0767, layer_0_sem_loss: 2.2960, layer_0_indi_loss: 1.8457, layer_1_score_loss: 0.0012, layer_1_mask_bce_loss: 0.0101, layer_1_mask_dice_loss: 0.0769, layer_1_sem_loss: 2.2882, layer_1_indi_loss: 1.6354, layer_2_score_loss: 0.0013, layer_2_mask_bce_loss: 0.0098, layer_2_mask_dice_loss: 0.0750, layer_2_sem_loss: 2.2840, layer_2_indi_loss: 1.6234, layer_3_score_loss: 0.0012, layer_3_mask_bce_loss: 0.0100, layer_3_mask_dice_loss: 0.0744, layer_3_sem_loss: 2.2926, layer_3_indi_loss: 1.6404, layer_4_score_loss: 0.0014, layer_4_mask_bce_loss: 0.0103, layer_4_mask_dice_loss: 0.0745, layer_4_sem_loss: 2.2936, layer_4_indi_loss: 1.6515, layer_5_score_loss: 0.0010, layer_5_mask_bce_loss: 0.0104, layer_5_mask_dice_loss: 0.0776, layer_5_sem_loss: 2.2906, layer_5_indi_loss: 1.6518, loss: 2.6375, grad_total_norm: 3.4175
2025-04-26 16:27:19,336 - INFO - Epoch [1/70][200/817]  lr: 0.0001, eta: 2 days, 21:13:54, data_time: 0.08, iter_time: 4.37, score_loss: 0.0006, mask_bce_loss: 0.0114, mask_dice_loss: 0.0788, sem_loss: 2.5953, indi_loss: 1.7065, sample_loss: 0.1441, layer_0_score_loss: 0.0011, layer_0_mask_bce_loss: 0.0112, layer_0_mask_dice_loss: 0.0773, layer_0_sem_loss: 2.5952, layer_0_indi_loss: 2.0090, layer_1_score_loss: 0.0008, layer_1_mask_bce_loss: 0.0120, layer_1_mask_dice_loss: 0.0782, layer_1_sem_loss: 2.6093, layer_1_indi_loss: 1.7304, layer_2_score_loss: 0.0007, layer_2_mask_bce_loss: 0.0132, layer_2_mask_dice_loss: 0.0768, layer_2_sem_loss: 2.5907, layer_2_indi_loss: 1.7188, layer_3_score_loss: 0.0008, layer_3_mask_bce_loss: 0.0136, layer_3_mask_dice_loss: 0.0757, layer_3_sem_loss: 2.5836, layer_3_indi_loss: 1.7142, layer_4_score_loss: 0.0004, layer_4_mask_bce_loss: 0.0135, layer_4_mask_dice_loss: 0.0748, layer_4_sem_loss: 2.5958, layer_4_indi_loss: 1.7040, layer_5_score_loss: 0.0006, layer_5_mask_bce_loss: 0.0127, layer_5_mask_dice_loss: 0.0762, layer_5_sem_loss: 2.5969, layer_5_indi_loss: 1.6992, loss: 2.8060, grad_total_norm: 5.1507
2025-04-26 16:28:02,853 - INFO - Epoch [1/70][210/817]  lr: 0.0001, eta: 2 days, 21:12:13, data_time: 0.08, iter_time: 4.37, score_loss: 0.0012, mask_bce_loss: 0.0115, mask_dice_loss: 0.0711, sem_loss: 2.6216, indi_loss: 1.5229, sample_loss: 0.1430, layer_0_score_loss: 0.0011, layer_0_mask_bce_loss: 0.0093, layer_0_mask_dice_loss: 0.0691, layer_0_sem_loss: 2.6187, layer_0_indi_loss: 1.7920, layer_1_score_loss: 0.0014, layer_1_mask_bce_loss: 0.0097, layer_1_mask_dice_loss: 0.0707, layer_1_sem_loss: 2.6229, layer_1_indi_loss: 1.5797, layer_2_score_loss: 0.0010, layer_2_mask_bce_loss: 0.0106, layer_2_mask_dice_loss: 0.0677, layer_2_sem_loss: 2.6200, layer_2_indi_loss: 1.5383, layer_3_score_loss: 0.0009, layer_3_mask_bce_loss: 0.0105, layer_3_mask_dice_loss: 0.0664, layer_3_sem_loss: 2.6226, layer_3_indi_loss: 1.5216, layer_4_score_loss: 0.0010, layer_4_mask_bce_loss: 0.0102, layer_4_mask_dice_loss: 0.0665, layer_4_sem_loss: 2.6234, layer_4_indi_loss: 1.5122, layer_5_score_loss: 0.0012, layer_5_mask_bce_loss: 0.0110, layer_5_mask_dice_loss: 0.0683, layer_5_sem_loss: 2.6198, layer_5_indi_loss: 1.5141, loss: 2.6988, grad_total_norm: 7.6929
2025-04-26 16:28:49,363 - INFO - Epoch [1/70][220/817]  lr: 0.0001, eta: 2 days, 21:23:31, data_time: 0.07, iter_time: 4.38, score_loss: 0.0004, mask_bce_loss: 0.0143, mask_dice_loss: 0.0776, sem_loss: 2.1133, indi_loss: 1.5120, sample_loss: 0.1433, layer_0_score_loss: 0.0003, layer_0_mask_bce_loss: 0.0118, layer_0_mask_dice_loss: 0.0782, layer_0_sem_loss: 2.1087, layer_0_indi_loss: 1.6898, layer_1_score_loss: 0.0005, layer_1_mask_bce_loss: 0.0123, layer_1_mask_dice_loss: 0.0785, layer_1_sem_loss: 2.1217, layer_1_indi_loss: 1.5605, layer_2_score_loss: 0.0006, layer_2_mask_bce_loss: 0.0129, layer_2_mask_dice_loss: 0.0760, layer_2_sem_loss: 2.1315, layer_2_indi_loss: 1.5243, layer_3_score_loss: 0.0006, layer_3_mask_bce_loss: 0.0141, layer_3_mask_dice_loss: 0.0762, layer_3_sem_loss: 2.1329, layer_3_indi_loss: 1.5146, layer_4_score_loss: 0.0006, layer_4_mask_bce_loss: 0.0151, layer_4_mask_dice_loss: 0.0760, layer_4_sem_loss: 2.1187, layer_4_indi_loss: 1.5132, layer_5_score_loss: 0.0002, layer_5_mask_bce_loss: 0.0145, layer_5_mask_dice_loss: 0.0772, layer_5_sem_loss: 2.1253, layer_5_indi_loss: 1.5149, loss: 2.5412, grad_total_norm: 11.5719
2025-04-26 16:29:32,228 - INFO - Epoch [1/70][230/817]  lr: 0.0001, eta: 2 days, 21:18:43, data_time: 0.07, iter_time: 4.38, score_loss: 0.0007, mask_bce_loss: 0.0128, mask_dice_loss: 0.1314, sem_loss: 3.0326, indi_loss: 1.8938, sample_loss: 0.1432, layer_0_score_loss: 0.0012, layer_0_mask_bce_loss: 0.0103, layer_0_mask_dice_loss: 0.1257, layer_0_sem_loss: 3.0482, layer_0_indi_loss: 2.4100, layer_1_score_loss: 0.0008, layer_1_mask_bce_loss: 0.0118, layer_1_mask_dice_loss: 0.1294, layer_1_sem_loss: 3.0331, layer_1_indi_loss: 1.9738, layer_2_score_loss: 0.0011, layer_2_mask_bce_loss: 0.0121, layer_2_mask_dice_loss: 0.1259, layer_2_sem_loss: 3.0303, layer_2_indi_loss: 1.9455, layer_3_score_loss: 0.0006, layer_3_mask_bce_loss: 0.0125, layer_3_mask_dice_loss: 0.1270, layer_3_sem_loss: 3.0370, layer_3_indi_loss: 1.9085, layer_4_score_loss: 0.0005, layer_4_mask_bce_loss: 0.0136, layer_4_mask_dice_loss: 0.1273, layer_4_sem_loss: 3.0379, layer_4_indi_loss: 1.8935, layer_5_score_loss: 0.0006, layer_5_mask_bce_loss: 0.0130, layer_5_mask_dice_loss: 0.1286, layer_5_sem_loss: 3.0347, layer_5_indi_loss: 1.8924, loss: 3.2678, grad_total_norm: 4.5434
2025-04-26 16:30:14,267 - INFO - Epoch [1/70][240/817]  lr: 0.0001, eta: 2 days, 21:11:00, data_time: 0.07, iter_time: 4.37, score_loss: 0.0010, mask_bce_loss: 0.0110, mask_dice_loss: 0.0860, sem_loss: 1.9001, indi_loss: 1.6831, sample_loss: 0.1440, layer_0_score_loss: 0.0008, layer_0_mask_bce_loss: 0.0102, layer_0_mask_dice_loss: 0.0868, layer_0_sem_loss: 1.8885, layer_0_indi_loss: 1.9844, layer_1_score_loss: 0.0011, layer_1_mask_bce_loss: 0.0099, layer_1_mask_dice_loss: 0.0879, layer_1_sem_loss: 1.8866, layer_1_indi_loss: 1.7553, layer_2_score_loss: 0.0010, layer_2_mask_bce_loss: 0.0105, layer_2_mask_dice_loss: 0.0848, layer_2_sem_loss: 1.8868, layer_2_indi_loss: 1.7002, layer_3_score_loss: 0.0009, layer_3_mask_bce_loss: 0.0104, layer_3_mask_dice_loss: 0.0837, layer_3_sem_loss: 1.8952, layer_3_indi_loss: 1.6772, layer_4_score_loss: 0.0011, layer_4_mask_bce_loss: 0.0109, layer_4_mask_dice_loss: 0.0837, layer_4_sem_loss: 1.8924, layer_4_indi_loss: 1.6755, layer_5_score_loss: 0.0009, layer_5_mask_bce_loss: 0.0105, layer_5_mask_dice_loss: 0.0847, layer_5_sem_loss: 1.8921, layer_5_indi_loss: 1.6750, loss: 2.5428, grad_total_norm: 5.8537
2025-04-26 16:30:59,423 - INFO - Epoch [1/70][250/817]  lr: 0.0001, eta: 2 days, 21:15:40, data_time: 0.06, iter_time: 4.38, score_loss: 0.0013, mask_bce_loss: 0.0211, mask_dice_loss: 0.1205, sem_loss: 2.9266, indi_loss: 2.0334, sample_loss: 0.1455, layer_0_score_loss: 0.0006, layer_0_mask_bce_loss: 0.0171, layer_0_mask_dice_loss: 0.1213, layer_0_sem_loss: 2.9466, layer_0_indi_loss: 2.4573, layer_1_score_loss: 0.0017, layer_1_mask_bce_loss: 0.0205, layer_1_mask_dice_loss: 0.1243, layer_1_sem_loss: 2.9683, layer_1_indi_loss: 2.0194, layer_2_score_loss: 0.0020, layer_2_mask_bce_loss: 0.0256, layer_2_mask_dice_loss: 0.1222, layer_2_sem_loss: 2.9559, layer_2_indi_loss: 1.9844, layer_3_score_loss: 0.0016, layer_3_mask_bce_loss: 0.0278, layer_3_mask_dice_loss: 0.1209, layer_3_sem_loss: 2.9748, layer_3_indi_loss: 2.0081, layer_4_score_loss: 0.0011, layer_4_mask_bce_loss: 0.0260, layer_4_mask_dice_loss: 0.1196, layer_4_sem_loss: 2.9584, layer_4_indi_loss: 2.0120, layer_5_score_loss: 0.0011, layer_5_mask_bce_loss: 0.0242, layer_5_mask_dice_loss: 0.1202, layer_5_sem_loss: 2.9331, layer_5_indi_loss: 2.0335, loss: 3.3044, grad_total_norm: 17.8217
2025-04-26 16:31:43,643 - INFO - Epoch [1/70][260/817]  lr: 0.0001, eta: 2 days, 21:16:30, data_time: 0.06, iter_time: 4.38, score_loss: 0.0015, mask_bce_loss: 0.0106, mask_dice_loss: 0.0360, sem_loss: 0.8101, indi_loss: 1.0344, sample_loss: 0.1392, layer_0_score_loss: 0.0006, layer_0_mask_bce_loss: 0.0080, layer_0_mask_dice_loss: 0.0373, layer_0_sem_loss: 0.8334, layer_0_indi_loss: 1.2252, layer_1_score_loss: 0.0010, layer_1_mask_bce_loss: 0.0093, layer_1_mask_dice_loss: 0.0372, layer_1_sem_loss: 0.8124, layer_1_indi_loss: 1.1081, layer_2_score_loss: 0.0014, layer_2_mask_bce_loss: 0.0103, layer_2_mask_dice_loss: 0.0353, layer_2_sem_loss: 0.8113, layer_2_indi_loss: 1.0322, layer_3_score_loss: 0.0014, layer_3_mask_bce_loss: 0.0106, layer_3_mask_dice_loss: 0.0351, layer_3_sem_loss: 0.8066, layer_3_indi_loss: 1.0172, layer_4_score_loss: 0.0013, layer_4_mask_bce_loss: 0.0110, layer_4_mask_dice_loss: 0.0349, layer_4_sem_loss: 0.8040, layer_4_indi_loss: 1.0175, layer_5_score_loss: 0.0012, layer_5_mask_bce_loss: 0.0110, layer_5_mask_dice_loss: 0.0355, layer_5_sem_loss: 0.8065, layer_5_indi_loss: 1.0326, loss: 1.6256, grad_total_norm: 10.8848
