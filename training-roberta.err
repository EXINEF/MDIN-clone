Using backend: pytorch
2025-04-26 16:22:20,619 - INFO - config: configs/default_3dgres.yaml
2025-04-26 16:22:20,624 - INFO - set random seed: 1999
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-04-26 16:22:24,749 - INFO - Parameters: 150.87M
2025-04-26 16:22:24,751 - INFO - Load pretrain from backbones/sp_unet_backbone.pth
2025-04-26 16:22:24,827 - WARNING - The model and loaded state dict do not match exactly

size mismatch for criterion.loss_weight: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([6]).
unexpected key in source state_dict: lang_proj.weight, lang_proj.bias, stm.x_mask.0.weight, stm.x_mask.0.bias, stm.x_mask.2.weight, stm.x_mask.2.bias

missing keys in source state_dict: bert_encoder.embeddings.word_embeddings.weight, bert_encoder.embeddings.position_embeddings.weight, bert_encoder.embeddings.token_type_embeddings.weight, bert_encoder.embeddings.LayerNorm.weight, bert_encoder.embeddings.LayerNorm.bias, bert_encoder.encoder.layer.0.attention.self.query.weight, bert_encoder.encoder.layer.0.attention.self.query.bias, bert_encoder.encoder.layer.0.attention.self.key.weight, bert_encoder.encoder.layer.0.attention.self.key.bias, bert_encoder.encoder.layer.0.attention.self.value.weight, bert_encoder.encoder.layer.0.attention.self.value.bias, bert_encoder.encoder.layer.0.attention.output.dense.weight, bert_encoder.encoder.layer.0.attention.output.dense.bias, bert_encoder.encoder.layer.0.attention.output.LayerNorm.weight, bert_encoder.encoder.layer.0.attention.output.LayerNorm.bias, bert_encoder.encoder.layer.0.intermediate.dense.weight, bert_encoder.encoder.layer.0.intermediate.dense.bias, bert_encoder.encoder.layer.0.output.dense.weight, bert_encoder.encoder.layer.0.output.dense.bias, bert_encoder.encoder.layer.0.output.LayerNorm.weight, bert_encoder.encoder.layer.0.output.LayerNorm.bias, bert_encoder.encoder.layer.1.attention.self.query.weight, bert_encoder.encoder.layer.1.attention.self.query.bias, bert_encoder.encoder.layer.1.attention.self.key.weight, bert_encoder.encoder.layer.1.attention.self.key.bias, bert_encoder.encoder.layer.1.attention.self.value.weight, bert_encoder.encoder.layer.1.attention.self.value.bias, bert_encoder.encoder.layer.1.attention.output.dense.weight, bert_encoder.encoder.layer.1.attention.output.dense.bias, bert_encoder.encoder.layer.1.attention.output.LayerNorm.weight, bert_encoder.encoder.layer.1.attention.output.LayerNorm.bias, bert_encoder.encoder.layer.1.intermediate.dense.weight, bert_encoder.encoder.layer.1.intermediate.dense.bias, bert_encoder.encoder.layer.1.output.dense.weight, bert_encoder.encoder.layer.1.output.dense.bias, bert_encoder.encoder.layer.1.output.LayerNorm.weight, bert_encoder.encoder.layer.1.output.LayerNorm.bias, bert_encoder.encoder.layer.2.attention.self.query.weight, bert_encoder.encoder.layer.2.attention.self.query.bias, bert_encoder.encoder.layer.2.attention.self.key.weight, bert_encoder.encoder.layer.2.attention.self.key.bias, bert_encoder.encoder.layer.2.attention.self.value.weight, bert_encoder.encoder.layer.2.attention.self.value.bias, bert_encoder.encoder.layer.2.attention.output.dense.weight, bert_encoder.encoder.layer.2.attention.output.dense.bias, bert_encoder.encoder.layer.2.attention.output.LayerNorm.weight, bert_encoder.encoder.layer.2.attention.output.LayerNorm.bias, bert_encoder.encoder.layer.2.intermediate.dense.weight, bert_encoder.encoder.layer.2.intermediate.dense.bias, bert_encoder.encoder.layer.2.output.dense.weight, bert_encoder.encoder.layer.2.output.dense.bias, bert_encoder.encoder.layer.2.output.LayerNorm.weight, bert_encoder.encoder.layer.2.output.LayerNorm.bias, bert_encoder.encoder.layer.3.attention.self.query.weight, bert_encoder.encoder.layer.3.attention.self.query.bias, bert_encoder.encoder.layer.3.attention.self.key.weight, bert_encoder.encoder.layer.3.attention.self.key.bias, bert_encoder.encoder.layer.3.attention.self.value.weight, bert_encoder.encoder.layer.3.attention.self.value.bias, bert_encoder.encoder.layer.3.attention.output.dense.weight, bert_encoder.encoder.layer.3.attention.output.dense.bias, bert_encoder.encoder.layer.3.attention.output.LayerNorm.weight, bert_encoder.encoder.layer.3.attention.output.LayerNorm.bias, bert_encoder.encoder.layer.3.intermediate.dense.weight, bert_encoder.encoder.layer.3.intermediate.dense.bias, bert_encoder.encoder.layer.3.output.dense.weight, bert_encoder.encoder.layer.3.output.dense.bias, bert_encoder.encoder.layer.3.output.LayerNorm.weight, bert_encoder.encoder.layer.3.output.LayerNorm.bias, bert_encoder.encoder.layer.4.attention.self.query.weight, bert_encoder.encoder.layer.4.attention.self.query.bias, bert_encoder.encoder.layer.4.attention.self.key.weight, bert_encoder.encoder.layer.4.attention.self.key.bias, bert_encoder.encoder.layer.4.attention.self.value.weight, bert_encoder.encoder.layer.4.attention.self.value.bias, bert_encoder.encoder.layer.4.attention.output.dense.weight, bert_encoder.encoder.layer.4.attention.output.dense.bias, bert_encoder.encoder.layer.4.attention.output.LayerNorm.weight, bert_encoder.encoder.layer.4.attention.output.LayerNorm.bias, bert_encoder.encoder.layer.4.intermediate.dense.weight, bert_encoder.encoder.layer.4.intermediate.dense.bias, bert_encoder.encoder.layer.4.output.dense.weight, bert_encoder.encoder.layer.4.output.dense.bias, bert_encoder.encoder.layer.4.output.LayerNorm.weight, bert_encoder.encoder.layer.4.output.LayerNorm.bias, bert_encoder.encoder.layer.5.attention.self.query.weight, bert_encoder.encoder.layer.5.attention.self.query.bias, bert_encoder.encoder.layer.5.attention.self.key.weight, bert_encoder.encoder.layer.5.attention.self.key.bias, bert_encoder.encoder.layer.5.attention.self.value.weight, bert_encoder.encoder.layer.5.attention.self.value.bias, bert_encoder.encoder.layer.5.attention.output.dense.weight, bert_encoder.encoder.layer.5.attention.output.dense.bias, bert_encoder.encoder.layer.5.attention.output.LayerNorm.weight, bert_encoder.encoder.layer.5.attention.output.LayerNorm.bias, bert_encoder.encoder.layer.5.intermediate.dense.weight, bert_encoder.encoder.layer.5.intermediate.dense.bias, bert_encoder.encoder.layer.5.output.dense.weight, bert_encoder.encoder.layer.5.output.dense.bias, bert_encoder.encoder.layer.5.output.LayerNorm.weight, bert_encoder.encoder.layer.5.output.LayerNorm.bias, bert_encoder.encoder.layer.6.attention.self.query.weight, bert_encoder.encoder.layer.6.attention.self.query.bias, bert_encoder.encoder.layer.6.attention.self.key.weight, bert_encoder.encoder.layer.6.attention.self.key.bias, bert_encoder.encoder.layer.6.attention.self.value.weight, bert_encoder.encoder.layer.6.attention.self.value.bias, bert_encoder.encoder.layer.6.attention.output.dense.weight, bert_encoder.encoder.layer.6.attention.output.dense.bias, bert_encoder.encoder.layer.6.attention.output.LayerNorm.weight, bert_encoder.encoder.layer.6.attention.output.LayerNorm.bias, bert_encoder.encoder.layer.6.intermediate.dense.weight, bert_encoder.encoder.layer.6.intermediate.dense.bias, bert_encoder.encoder.layer.6.output.dense.weight, bert_encoder.encoder.layer.6.output.dense.bias, bert_encoder.encoder.layer.6.output.LayerNorm.weight, bert_encoder.encoder.layer.6.output.LayerNorm.bias, bert_encoder.encoder.layer.7.attention.self.query.weight, bert_encoder.encoder.layer.7.attention.self.query.bias, bert_encoder.encoder.layer.7.attention.self.key.weight, bert_encoder.encoder.layer.7.attention.self.key.bias, bert_encoder.encoder.layer.7.attention.self.value.weight, bert_encoder.encoder.layer.7.attention.self.value.bias, bert_encoder.encoder.layer.7.attention.output.dense.weight, bert_encoder.encoder.layer.7.attention.output.dense.bias, bert_encoder.encoder.layer.7.attention.output.LayerNorm.weight, bert_encoder.encoder.layer.7.attention.output.LayerNorm.bias, bert_encoder.encoder.layer.7.intermediate.dense.weight, bert_encoder.encoder.layer.7.intermediate.dense.bias, bert_encoder.encoder.layer.7.output.dense.weight, bert_encoder.encoder.layer.7.output.dense.bias, bert_encoder.encoder.layer.7.output.LayerNorm.weight, bert_encoder.encoder.layer.7.output.LayerNorm.bias, bert_encoder.encoder.layer.8.attention.self.query.weight, bert_encoder.encoder.layer.8.attention.self.query.bias, bert_encoder.encoder.layer.8.attention.self.key.weight, bert_encoder.encoder.layer.8.attention.self.key.bias, bert_encoder.encoder.layer.8.attention.self.value.weight, bert_encoder.encoder.layer.8.attention.self.value.bias, bert_encoder.encoder.layer.8.attention.output.dense.weight, bert_encoder.encoder.layer.8.attention.output.dense.bias, bert_encoder.encoder.layer.8.attention.output.LayerNorm.weight, bert_encoder.encoder.layer.8.attention.output.LayerNorm.bias, bert_encoder.encoder.layer.8.intermediate.dense.weight, bert_encoder.encoder.layer.8.intermediate.dense.bias, bert_encoder.encoder.layer.8.output.dense.weight, bert_encoder.encoder.layer.8.output.dense.bias, bert_encoder.encoder.layer.8.output.LayerNorm.weight, bert_encoder.encoder.layer.8.output.LayerNorm.bias, bert_encoder.encoder.layer.9.attention.self.query.weight, bert_encoder.encoder.layer.9.attention.self.query.bias, bert_encoder.encoder.layer.9.attention.self.key.weight, bert_encoder.encoder.layer.9.attention.self.key.bias, bert_encoder.encoder.layer.9.attention.self.value.weight, bert_encoder.encoder.layer.9.attention.self.value.bias, bert_encoder.encoder.layer.9.attention.output.dense.weight, bert_encoder.encoder.layer.9.attention.output.dense.bias, bert_encoder.encoder.layer.9.attention.output.LayerNorm.weight, bert_encoder.encoder.layer.9.attention.output.LayerNorm.bias, bert_encoder.encoder.layer.9.intermediate.dense.weight, bert_encoder.encoder.layer.9.intermediate.dense.bias, bert_encoder.encoder.layer.9.output.dense.weight, bert_encoder.encoder.layer.9.output.dense.bias, bert_encoder.encoder.layer.9.output.LayerNorm.weight, bert_encoder.encoder.layer.9.output.LayerNorm.bias, bert_encoder.encoder.layer.10.attention.self.query.weight, bert_encoder.encoder.layer.10.attention.self.query.bias, bert_encoder.encoder.layer.10.attention.self.key.weight, bert_encoder.encoder.layer.10.attention.self.key.bias, bert_encoder.encoder.layer.10.attention.self.value.weight, bert_encoder.encoder.layer.10.attention.self.value.bias, bert_encoder.encoder.layer.10.attention.output.dense.weight, bert_encoder.encoder.layer.10.attention.output.dense.bias, bert_encoder.encoder.layer.10.attention.output.LayerNorm.weight, bert_encoder.encoder.layer.10.attention.output.LayerNorm.bias, bert_encoder.encoder.layer.10.intermediate.dense.weight, bert_encoder.encoder.layer.10.intermediate.dense.bias, bert_encoder.encoder.layer.10.output.dense.weight, bert_encoder.encoder.layer.10.output.dense.bias, bert_encoder.encoder.layer.10.output.LayerNorm.weight, bert_encoder.encoder.layer.10.output.LayerNorm.bias, bert_encoder.encoder.layer.11.attention.self.query.weight, bert_encoder.encoder.layer.11.attention.self.query.bias, bert_encoder.encoder.layer.11.attention.self.key.weight, bert_encoder.encoder.layer.11.attention.self.key.bias, bert_encoder.encoder.layer.11.attention.self.value.weight, bert_encoder.encoder.layer.11.attention.self.value.bias, bert_encoder.encoder.layer.11.attention.output.dense.weight, bert_encoder.encoder.layer.11.attention.output.dense.bias, bert_encoder.encoder.layer.11.attention.output.LayerNorm.weight, bert_encoder.encoder.layer.11.attention.output.LayerNorm.bias, bert_encoder.encoder.layer.11.intermediate.dense.weight, bert_encoder.encoder.layer.11.intermediate.dense.bias, bert_encoder.encoder.layer.11.output.dense.weight, bert_encoder.encoder.layer.11.output.dense.bias, bert_encoder.encoder.layer.11.output.LayerNorm.weight, bert_encoder.encoder.layer.11.output.LayerNorm.bias, bert_encoder.pooler.dense.weight, bert_encoder.pooler.dense.bias, mdin.input_proj.0.weight, mdin.input_proj.0.bias, mdin.input_proj.1.weight, mdin.input_proj.1.bias, mdin.lang_proj.weight, mdin.lang_proj.bias, mdin.lang_norm.weight, mdin.lang_norm.bias, mdin.sampling_module.camap.query_proj.weight, mdin.sampling_module.camap.key_proj.weight, mdin.query_generator.0.weight, mdin.query_generator.0.bias, mdin.query_generator.2.weight, mdin.query_generator.2.bias, mdin.query_generator.4.weight, mdin.query_generator.4.bias, mdin.qsa_layers.0.attn.in_proj_weight, mdin.qsa_layers.0.attn.in_proj_bias, mdin.qsa_layers.0.attn.out_proj.weight, mdin.qsa_layers.0.attn.out_proj.bias, mdin.qsa_layers.0.norm.weight, mdin.qsa_layers.0.norm.bias, mdin.qsa_layers.1.attn.in_proj_weight, mdin.qsa_layers.1.attn.in_proj_bias, mdin.qsa_layers.1.attn.out_proj.weight, mdin.qsa_layers.1.attn.out_proj.bias, mdin.qsa_layers.1.norm.weight, mdin.qsa_layers.1.norm.bias, mdin.qsa_layers.2.attn.in_proj_weight, mdin.qsa_layers.2.attn.in_proj_bias, mdin.qsa_layers.2.attn.out_proj.weight, mdin.qsa_layers.2.attn.out_proj.bias, mdin.qsa_layers.2.norm.weight, mdin.qsa_layers.2.norm.bias, mdin.qsa_layers.3.attn.in_proj_weight, mdin.qsa_layers.3.attn.in_proj_bias, mdin.qsa_layers.3.attn.out_proj.weight, mdin.qsa_layers.3.attn.out_proj.bias, mdin.qsa_layers.3.norm.weight, mdin.qsa_layers.3.norm.bias, mdin.qsa_layers.4.attn.in_proj_weight, mdin.qsa_layers.4.attn.in_proj_bias, mdin.qsa_layers.4.attn.out_proj.weight, mdin.qsa_layers.4.attn.out_proj.bias, mdin.qsa_layers.4.norm.weight, mdin.qsa_layers.4.norm.bias, mdin.qsa_layers.5.attn.in_proj_weight, mdin.qsa_layers.5.attn.in_proj_bias, mdin.qsa_layers.5.attn.out_proj.weight, mdin.qsa_layers.5.attn.out_proj.bias, mdin.qsa_layers.5.norm.weight, mdin.qsa_layers.5.norm.bias, mdin.qqa_layers.0.attn.in_proj_weight, mdin.qqa_layers.0.attn.in_proj_bias, mdin.qqa_layers.0.attn.out_proj.weight, mdin.qqa_layers.0.attn.out_proj.bias, mdin.qqa_layers.0.norm.weight, mdin.qqa_layers.0.norm.bias, mdin.qqa_layers.1.attn.in_proj_weight, mdin.qqa_layers.1.attn.in_proj_bias, mdin.qqa_layers.1.attn.out_proj.weight, mdin.qqa_layers.1.attn.out_proj.bias, mdin.qqa_layers.1.norm.weight, mdin.qqa_layers.1.norm.bias, mdin.qqa_layers.2.attn.in_proj_weight, mdin.qqa_layers.2.attn.in_proj_bias, mdin.qqa_layers.2.attn.out_proj.weight, mdin.qqa_layers.2.attn.out_proj.bias, mdin.qqa_layers.2.norm.weight, mdin.qqa_layers.2.norm.bias, mdin.qqa_layers.3.attn.in_proj_weight, mdin.qqa_layers.3.attn.in_proj_bias, mdin.qqa_layers.3.attn.out_proj.weight, mdin.qqa_layers.3.attn.out_proj.bias, mdin.qqa_layers.3.norm.weight, mdin.qqa_layers.3.norm.bias, mdin.qqa_layers.4.attn.in_proj_weight, mdin.qqa_layers.4.attn.in_proj_bias, mdin.qqa_layers.4.attn.out_proj.weight, mdin.qqa_layers.4.attn.out_proj.bias, mdin.qqa_layers.4.norm.weight, mdin.qqa_layers.4.norm.bias, mdin.qqa_layers.5.attn.in_proj_weight, mdin.qqa_layers.5.attn.in_proj_bias, mdin.qqa_layers.5.attn.out_proj.weight, mdin.qqa_layers.5.attn.out_proj.bias, mdin.qqa_layers.5.norm.weight, mdin.qqa_layers.5.norm.bias, mdin.qla_layers.0.attn.in_proj_weight, mdin.qla_layers.0.attn.in_proj_bias, mdin.qla_layers.0.attn.out_proj.weight, mdin.qla_layers.0.attn.out_proj.bias, mdin.qla_layers.0.norm.weight, mdin.qla_layers.0.norm.bias, mdin.qla_layers.1.attn.in_proj_weight, mdin.qla_layers.1.attn.in_proj_bias, mdin.qla_layers.1.attn.out_proj.weight, mdin.qla_layers.1.attn.out_proj.bias, mdin.qla_layers.1.norm.weight, mdin.qla_layers.1.norm.bias, mdin.qla_layers.2.attn.in_proj_weight, mdin.qla_layers.2.attn.in_proj_bias, mdin.qla_layers.2.attn.out_proj.weight, mdin.qla_layers.2.attn.out_proj.bias, mdin.qla_layers.2.norm.weight, mdin.qla_layers.2.norm.bias, mdin.qla_layers.3.attn.in_proj_weight, mdin.qla_layers.3.attn.in_proj_bias, mdin.qla_layers.3.attn.out_proj.weight, mdin.qla_layers.3.attn.out_proj.bias, mdin.qla_layers.3.norm.weight, mdin.qla_layers.3.norm.bias, mdin.qla_layers.4.attn.in_proj_weight, mdin.qla_layers.4.attn.in_proj_bias, mdin.qla_layers.4.attn.out_proj.weight, mdin.qla_layers.4.attn.out_proj.bias, mdin.qla_layers.4.norm.weight, mdin.qla_layers.4.norm.bias, mdin.qla_layers.5.attn.in_proj_weight, mdin.qla_layers.5.attn.in_proj_bias, mdin.qla_layers.5.attn.out_proj.weight, mdin.qla_layers.5.attn.out_proj.bias, mdin.qla_layers.5.norm.weight, mdin.qla_layers.5.norm.bias, mdin.qla_ffn_layers.0.net.0.weight, mdin.qla_ffn_layers.0.net.0.bias, mdin.qla_ffn_layers.0.net.3.weight, mdin.qla_ffn_layers.0.net.3.bias, mdin.qla_ffn_layers.0.norm.weight, mdin.qla_ffn_layers.0.norm.bias, mdin.qla_ffn_layers.1.net.0.weight, mdin.qla_ffn_layers.1.net.0.bias, mdin.qla_ffn_layers.1.net.3.weight, mdin.qla_ffn_layers.1.net.3.bias, mdin.qla_ffn_layers.1.norm.weight, mdin.qla_ffn_layers.1.norm.bias, mdin.qla_ffn_layers.2.net.0.weight, mdin.qla_ffn_layers.2.net.0.bias, mdin.qla_ffn_layers.2.net.3.weight, mdin.qla_ffn_layers.2.net.3.bias, mdin.qla_ffn_layers.2.norm.weight, mdin.qla_ffn_layers.2.norm.bias, mdin.qla_ffn_layers.3.net.0.weight, mdin.qla_ffn_layers.3.net.0.bias, mdin.qla_ffn_layers.3.net.3.weight, mdin.qla_ffn_layers.3.net.3.bias, mdin.qla_ffn_layers.3.norm.weight, mdin.qla_ffn_layers.3.norm.bias, mdin.qla_ffn_layers.4.net.0.weight, mdin.qla_ffn_layers.4.net.0.bias, mdin.qla_ffn_layers.4.net.3.weight, mdin.qla_ffn_layers.4.net.3.bias, mdin.qla_ffn_layers.4.norm.weight, mdin.qla_ffn_layers.4.norm.bias, mdin.qla_ffn_layers.5.net.0.weight, mdin.qla_ffn_layers.5.net.0.bias, mdin.qla_ffn_layers.5.net.3.weight, mdin.qla_ffn_layers.5.net.3.bias, mdin.qla_ffn_layers.5.norm.weight, mdin.qla_ffn_layers.5.norm.bias, mdin.lla_layers.0.attn.in_proj_weight, mdin.lla_layers.0.attn.in_proj_bias, mdin.lla_layers.0.attn.out_proj.weight, mdin.lla_layers.0.attn.out_proj.bias, mdin.lla_layers.0.norm.weight, mdin.lla_layers.0.norm.bias, mdin.lla_layers.1.attn.in_proj_weight, mdin.lla_layers.1.attn.in_proj_bias, mdin.lla_layers.1.attn.out_proj.weight, mdin.lla_layers.1.attn.out_proj.bias, mdin.lla_layers.1.norm.weight, mdin.lla_layers.1.norm.bias, mdin.lla_layers.2.attn.in_proj_weight, mdin.lla_layers.2.attn.in_proj_bias, mdin.lla_layers.2.attn.out_proj.weight, mdin.lla_layers.2.attn.out_proj.bias, mdin.lla_layers.2.norm.weight, mdin.lla_layers.2.norm.bias, mdin.lla_layers.3.attn.in_proj_weight, mdin.lla_layers.3.attn.in_proj_bias, mdin.lla_layers.3.attn.out_proj.weight, mdin.lla_layers.3.attn.out_proj.bias, mdin.lla_layers.3.norm.weight, mdin.lla_layers.3.norm.bias, mdin.lla_layers.4.attn.in_proj_weight, mdin.lla_layers.4.attn.in_proj_bias, mdin.lla_layers.4.attn.out_proj.weight, mdin.lla_layers.4.attn.out_proj.bias, mdin.lla_layers.4.norm.weight, mdin.lla_layers.4.norm.bias, mdin.lla_layers.5.attn.in_proj_weight, mdin.lla_layers.5.attn.in_proj_bias, mdin.lla_layers.5.attn.out_proj.weight, mdin.lla_layers.5.attn.out_proj.bias, mdin.lla_layers.5.norm.weight, mdin.lla_layers.5.norm.bias, mdin.lsa_layers.0.attn.in_proj_weight, mdin.lsa_layers.0.attn.in_proj_bias, mdin.lsa_layers.0.attn.out_proj.weight, mdin.lsa_layers.0.attn.out_proj.bias, mdin.lsa_layers.0.norm.weight, mdin.lsa_layers.0.norm.bias, mdin.lsa_layers.1.attn.in_proj_weight, mdin.lsa_layers.1.attn.in_proj_bias, mdin.lsa_layers.1.attn.out_proj.weight, mdin.lsa_layers.1.attn.out_proj.bias, mdin.lsa_layers.1.norm.weight, mdin.lsa_layers.1.norm.bias, mdin.lsa_layers.2.attn.in_proj_weight, mdin.lsa_layers.2.attn.in_proj_bias, mdin.lsa_layers.2.attn.out_proj.weight, mdin.lsa_layers.2.attn.out_proj.bias, mdin.lsa_layers.2.norm.weight, mdin.lsa_layers.2.norm.bias, mdin.lsa_layers.3.attn.in_proj_weight, mdin.lsa_layers.3.attn.in_proj_bias, mdin.lsa_layers.3.attn.out_proj.weight, mdin.lsa_layers.3.attn.out_proj.bias, mdin.lsa_layers.3.norm.weight, mdin.lsa_layers.3.norm.bias, mdin.lsa_layers.4.attn.in_proj_weight, mdin.lsa_layers.4.attn.in_proj_bias, mdin.lsa_layers.4.attn.out_proj.weight, mdin.lsa_layers.4.attn.out_proj.bias, mdin.lsa_layers.4.norm.weight, mdin.lsa_layers.4.norm.bias, mdin.lsa_layers.5.attn.in_proj_weight, mdin.lsa_layers.5.attn.in_proj_bias, mdin.lsa_layers.5.attn.out_proj.weight, mdin.lsa_layers.5.attn.out_proj.bias, mdin.lsa_layers.5.norm.weight, mdin.lsa_layers.5.norm.bias, mdin.lsa_ffn_layers.0.net.0.weight, mdin.lsa_ffn_layers.0.net.0.bias, mdin.lsa_ffn_layers.0.net.3.weight, mdin.lsa_ffn_layers.0.net.3.bias, mdin.lsa_ffn_layers.0.norm.weight, mdin.lsa_ffn_layers.0.norm.bias, mdin.lsa_ffn_layers.1.net.0.weight, mdin.lsa_ffn_layers.1.net.0.bias, mdin.lsa_ffn_layers.1.net.3.weight, mdin.lsa_ffn_layers.1.net.3.bias, mdin.lsa_ffn_layers.1.norm.weight, mdin.lsa_ffn_layers.1.norm.bias, mdin.lsa_ffn_layers.2.net.0.weight, mdin.lsa_ffn_layers.2.net.0.bias, mdin.lsa_ffn_layers.2.net.3.weight, mdin.lsa_ffn_layers.2.net.3.bias, mdin.lsa_ffn_layers.2.norm.weight, mdin.lsa_ffn_layers.2.norm.bias, mdin.lsa_ffn_layers.3.net.0.weight, mdin.lsa_ffn_layers.3.net.0.bias, mdin.lsa_ffn_layers.3.net.3.weight, mdin.lsa_ffn_layers.3.net.3.bias, mdin.lsa_ffn_layers.3.norm.weight, mdin.lsa_ffn_layers.3.norm.bias, mdin.lsa_ffn_layers.4.net.0.weight, mdin.lsa_ffn_layers.4.net.0.bias, mdin.lsa_ffn_layers.4.net.3.weight, mdin.lsa_ffn_layers.4.net.3.bias, mdin.lsa_ffn_layers.4.norm.weight, mdin.lsa_ffn_layers.4.norm.bias, mdin.lsa_ffn_layers.5.net.0.weight, mdin.lsa_ffn_layers.5.net.0.bias, mdin.lsa_ffn_layers.5.net.3.weight, mdin.lsa_ffn_layers.5.net.3.bias, mdin.lsa_ffn_layers.5.norm.weight, mdin.lsa_ffn_layers.5.norm.bias, mdin.out_norm.weight, mdin.out_norm.bias, mdin.out_score.0.weight, mdin.out_score.0.bias, mdin.out_score.2.weight, mdin.out_score.2.bias, mdin.x_mask.0.weight, mdin.x_mask.0.bias, mdin.x_mask.2.weight, mdin.x_mask.2.bias, mdin.indi_embedding.0.weight, mdin.indi_embedding.0.bias, mdin.indi_embedding.2.weight, mdin.indi_embedding.2.bias, mdin.indi_embedding.3.weight, mdin.indi_embedding.3.bias, mdin.indi_norm.weight, mdin.indi_norm.bias, mdin.contrastive_align_projection_vision.0.weight, mdin.contrastive_align_projection_vision.0.bias, mdin.contrastive_align_projection_vision.2.weight, mdin.contrastive_align_projection_vision.2.bias, mdin.contrastive_align_projection_vision.4.weight, mdin.contrastive_align_projection_vision.4.bias, mdin.contrastive_align_projection_text.0.weight, mdin.contrastive_align_projection_text.0.bias, mdin.contrastive_align_projection_text.2.weight, mdin.contrastive_align_projection_text.2.bias, mdin.contrastive_align_projection_text.4.weight, mdin.contrastive_align_projection_text.4.bias, criterion.matcher.cost_weight

2025-04-26 16:22:24,827 - INFO - Train batch size per gpu: 7
2025-04-26 16:22:25,043 - INFO - Load train multi3drefer: 43838 samples
2025-04-26 16:22:29,182 - INFO - Load val multi3drefer: 11120 samples
2025-04-26 16:22:30,425 - INFO - Training
tools/train_3dgres.py:85: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
2025-04-26 16:23:35,281 - INFO - Epoch [1/70][10/817]  lr: 0.0001, eta: 4 days, 6:59:59, data_time: 1.42, iter_time: 6.48, score_loss: 0.0004, mask_bce_loss: 0.0263, mask_dice_loss: 0.0706, sem_loss: 4.3413, indi_loss: 1.4551, sample_loss: 0.1424, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0329, layer_0_mask_dice_loss: 0.0703, layer_0_sem_loss: 4.2839, layer_0_indi_loss: 1.4247, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0277, layer_1_mask_dice_loss: 0.0716, layer_1_sem_loss: 4.2448, layer_1_indi_loss: 1.4499, layer_2_score_loss: 0.0006, layer_2_mask_bce_loss: 0.0247, layer_2_mask_dice_loss: 0.0693, layer_2_sem_loss: 4.3010, layer_2_indi_loss: 1.4606, layer_3_score_loss: 0.0004, layer_3_mask_bce_loss: 0.0227, layer_3_mask_dice_loss: 0.0695, layer_3_sem_loss: 4.3131, layer_3_indi_loss: 1.4571, layer_4_score_loss: 0.0004, layer_4_mask_bce_loss: 0.0222, layer_4_mask_dice_loss: 0.0705, layer_4_sem_loss: 4.3185, layer_4_indi_loss: 1.4637, layer_5_score_loss: 0.0010, layer_5_mask_bce_loss: 0.0224, layer_5_mask_dice_loss: 0.0706, layer_5_sem_loss: 4.3213, layer_5_indi_loss: 1.4579, loss: 3.4020, grad_total_norm: 3.3822
2025-04-26 16:24:20,861 - INFO - Epoch [1/70][20/817]  lr: 0.0001, eta: 3 days, 15:40:58, data_time: 0.71, iter_time: 5.52, score_loss: 0.0000, mask_bce_loss: 0.0167, mask_dice_loss: 0.1568, sem_loss: 6.7319, indi_loss: 1.9653, sample_loss: 0.1448, layer_0_score_loss: 0.0004, layer_0_mask_bce_loss: 0.0143, layer_0_mask_dice_loss: 0.1570, layer_0_sem_loss: 6.7283, layer_0_indi_loss: 2.0488, layer_1_score_loss: 0.0001, layer_1_mask_bce_loss: 0.0187, layer_1_mask_dice_loss: 0.1600, layer_1_sem_loss: 6.7608, layer_1_indi_loss: 1.9763, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0168, layer_2_mask_dice_loss: 0.1570, layer_2_sem_loss: 6.7635, layer_2_indi_loss: 1.9687, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0167, layer_3_mask_dice_loss: 0.1567, layer_3_sem_loss: 6.7523, layer_3_indi_loss: 1.9652, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0150, layer_4_mask_dice_loss: 0.1572, layer_4_sem_loss: 6.7439, layer_4_indi_loss: 1.9628, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0139, layer_5_mask_dice_loss: 0.1587, layer_5_sem_loss: 6.7334, layer_5_indi_loss: 1.9644, loss: 4.9020, grad_total_norm: 2.0867
2025-04-26 16:25:04,117 - INFO - Epoch [1/70][30/817]  lr: 0.0001, eta: 3 days, 9:20:16, data_time: 0.47, iter_time: 5.12, score_loss: 0.0000, mask_bce_loss: 0.0148, mask_dice_loss: 0.1078, sem_loss: 3.4861, indi_loss: 1.6868, sample_loss: 0.1448, layer_0_score_loss: 0.0003, layer_0_mask_bce_loss: 0.0152, layer_0_mask_dice_loss: 0.1043, layer_0_sem_loss: 3.6245, layer_0_indi_loss: 1.6894, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0240, layer_1_mask_dice_loss: 0.1071, layer_1_sem_loss: 3.4798, layer_1_indi_loss: 1.6825, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0239, layer_2_mask_dice_loss: 0.1070, layer_2_sem_loss: 3.4558, layer_2_indi_loss: 1.6828, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0173, layer_3_mask_dice_loss: 0.1073, layer_3_sem_loss: 3.4516, layer_3_indi_loss: 1.6800, layer_4_score_loss: 0.0001, layer_4_mask_bce_loss: 0.0178, layer_4_mask_dice_loss: 0.1061, layer_4_sem_loss: 3.4578, layer_4_indi_loss: 1.6830, layer_5_score_loss: 0.0001, layer_5_mask_bce_loss: 0.0175, layer_5_mask_dice_loss: 0.1055, layer_5_sem_loss: 3.4888, layer_5_indi_loss: 1.6849, loss: 3.2873, grad_total_norm: 6.4302
2025-04-26 16:25:46,409 - INFO - Epoch [1/70][40/817]  lr: 0.0001, eta: 3 days, 5:46:40, data_time: 0.36, iter_time: 4.90, score_loss: 0.0000, mask_bce_loss: 0.0166, mask_dice_loss: 0.1293, sem_loss: 3.9801, indi_loss: 1.8158, sample_loss: 0.1438, layer_0_score_loss: 0.0001, layer_0_mask_bce_loss: 0.0203, layer_0_mask_dice_loss: 0.1208, layer_0_sem_loss: 3.9662, layer_0_indi_loss: 1.8510, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0165, layer_1_mask_dice_loss: 0.1260, layer_1_sem_loss: 3.9620, layer_1_indi_loss: 1.8130, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0166, layer_2_mask_dice_loss: 0.1266, layer_2_sem_loss: 4.0004, layer_2_indi_loss: 1.8115, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0226, layer_3_mask_dice_loss: 0.1260, layer_3_sem_loss: 3.9271, layer_3_indi_loss: 1.8159, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0166, layer_4_mask_dice_loss: 0.1280, layer_4_sem_loss: 3.9601, layer_4_indi_loss: 1.8131, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0160, layer_5_mask_dice_loss: 0.1284, layer_5_sem_loss: 3.9779, layer_5_indi_loss: 1.8137, loss: 3.6126, grad_total_norm: 12.8824
2025-04-26 16:26:30,496 - INFO - Epoch [1/70][50/817]  lr: 0.0001, eta: 3 days, 4:12:23, data_time: 0.29, iter_time: 4.80, score_loss: 0.0000, mask_bce_loss: 0.0214, mask_dice_loss: 0.1143, sem_loss: 3.2582, indi_loss: 1.7353, sample_loss: 0.1443, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0165, layer_0_mask_dice_loss: 0.1102, layer_0_sem_loss: 3.3440, layer_0_indi_loss: 1.7477, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0213, layer_1_mask_dice_loss: 0.1135, layer_1_sem_loss: 3.2516, layer_1_indi_loss: 1.7291, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0212, layer_2_mask_dice_loss: 0.1143, layer_2_sem_loss: 3.2664, layer_2_indi_loss: 1.7313, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0224, layer_3_mask_dice_loss: 0.1145, layer_3_sem_loss: 3.2785, layer_3_indi_loss: 1.7336, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0228, layer_4_mask_dice_loss: 0.1151, layer_4_sem_loss: 3.2516, layer_4_indi_loss: 1.7319, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0224, layer_5_mask_dice_loss: 0.1140, layer_5_sem_loss: 3.2438, layer_5_indi_loss: 1.7321, loss: 3.2630, grad_total_norm: 16.2660
2025-04-26 16:27:15,530 - INFO - Epoch [1/70][60/817]  lr: 0.0001, eta: 3 days, 3:24:19, data_time: 0.24, iter_time: 4.75, score_loss: 0.0000, mask_bce_loss: 0.0135, mask_dice_loss: 0.0893, sem_loss: 2.1703, indi_loss: 1.6150, sample_loss: 0.1444, layer_0_score_loss: 0.0002, layer_0_mask_bce_loss: 0.0109, layer_0_mask_dice_loss: 0.0817, layer_0_sem_loss: 2.2741, layer_0_indi_loss: 1.6214, layer_1_score_loss: 0.0001, layer_1_mask_bce_loss: 0.0130, layer_1_mask_dice_loss: 0.0847, layer_1_sem_loss: 2.1860, layer_1_indi_loss: 1.6169, layer_2_score_loss: 0.0001, layer_2_mask_bce_loss: 0.0120, layer_2_mask_dice_loss: 0.0860, layer_2_sem_loss: 2.1765, layer_2_indi_loss: 1.6188, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0126, layer_3_mask_dice_loss: 0.0866, layer_3_sem_loss: 2.1765, layer_3_indi_loss: 1.6141, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0133, layer_4_mask_dice_loss: 0.0882, layer_4_sem_loss: 2.1828, layer_4_indi_loss: 1.6156, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0137, layer_5_mask_dice_loss: 0.0891, layer_5_sem_loss: 2.1843, layer_5_indi_loss: 1.6158, loss: 2.6446, grad_total_norm: 5.3765
2025-04-26 16:28:00,051 - INFO - Epoch [1/70][70/817]  lr: 0.0001, eta: 3 days, 2:42:46, data_time: 0.20, iter_time: 4.71, score_loss: 0.0000, mask_bce_loss: 0.0191, mask_dice_loss: 0.0906, sem_loss: 2.4527, indi_loss: 1.5955, sample_loss: 0.1442, layer_0_score_loss: 0.0002, layer_0_mask_bce_loss: 0.0145, layer_0_mask_dice_loss: 0.0818, layer_0_sem_loss: 2.5450, layer_0_indi_loss: 1.5915, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0184, layer_1_mask_dice_loss: 0.0892, layer_1_sem_loss: 2.4633, layer_1_indi_loss: 1.5970, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0189, layer_2_mask_dice_loss: 0.0898, layer_2_sem_loss: 2.4525, layer_2_indi_loss: 1.5970, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0195, layer_3_mask_dice_loss: 0.0906, layer_3_sem_loss: 2.4364, layer_3_indi_loss: 1.5941, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0180, layer_4_mask_dice_loss: 0.0899, layer_4_sem_loss: 2.4411, layer_4_indi_loss: 1.5966, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0203, layer_5_mask_dice_loss: 0.0915, layer_5_sem_loss: 2.4477, layer_5_indi_loss: 1.5960, loss: 2.7762, grad_total_norm: 6.6133
2025-04-26 16:28:43,338 - INFO - Epoch [1/70][80/817]  lr: 0.0001, eta: 3 days, 1:56:45, data_time: 0.18, iter_time: 4.66, score_loss: 0.0000, mask_bce_loss: 0.0113, mask_dice_loss: 0.0655, sem_loss: 1.8834, indi_loss: 1.3742, sample_loss: 0.1425, layer_0_score_loss: 0.0001, layer_0_mask_bce_loss: 0.0091, layer_0_mask_dice_loss: 0.0594, layer_0_sem_loss: 1.9389, layer_0_indi_loss: 1.3290, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0111, layer_1_mask_dice_loss: 0.0645, layer_1_sem_loss: 1.8883, layer_1_indi_loss: 1.3668, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0113, layer_2_mask_dice_loss: 0.0646, layer_2_sem_loss: 1.8865, layer_2_indi_loss: 1.3715, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0113, layer_3_mask_dice_loss: 0.0656, layer_3_sem_loss: 1.8879, layer_3_indi_loss: 1.3645, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0116, layer_4_mask_dice_loss: 0.0660, layer_4_sem_loss: 1.8746, layer_4_indi_loss: 1.3741, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0112, layer_5_mask_dice_loss: 0.0648, layer_5_sem_loss: 1.8739, layer_5_indi_loss: 1.3775, loss: 2.3196, grad_total_norm: 12.7684
2025-04-26 16:29:29,920 - INFO - Epoch [1/70][90/817]  lr: 0.0001, eta: 3 days, 1:55:40, data_time: 0.16, iter_time: 4.66, score_loss: 0.0000, mask_bce_loss: 0.0165, mask_dice_loss: 0.1496, sem_loss: 3.4426, indi_loss: 2.0789, sample_loss: 0.1440, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0118, layer_0_mask_dice_loss: 0.1342, layer_0_sem_loss: 3.3580, layer_0_indi_loss: 2.1665, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0165, layer_1_mask_dice_loss: 0.1478, layer_1_sem_loss: 3.3499, layer_1_indi_loss: 2.0780, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0158, layer_2_mask_dice_loss: 0.1477, layer_2_sem_loss: 3.3738, layer_2_indi_loss: 2.0734, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0157, layer_3_mask_dice_loss: 0.1467, layer_3_sem_loss: 3.3779, layer_3_indi_loss: 2.0789, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0156, layer_4_mask_dice_loss: 0.1463, layer_4_sem_loss: 3.3838, layer_4_indi_loss: 2.0717, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0151, layer_5_mask_dice_loss: 0.1481, layer_5_sem_loss: 3.4452, layer_5_indi_loss: 2.0749, loss: 3.5663, grad_total_norm: 16.8520
2025-04-26 16:30:15,797 - INFO - Epoch [1/70][100/817]  lr: 0.0001, eta: 3 days, 1:47:55, data_time: 0.14, iter_time: 4.65, score_loss: 0.0000, mask_bce_loss: 0.0221, mask_dice_loss: 0.1590, sem_loss: 3.4520, indi_loss: 2.1275, sample_loss: 0.1448, layer_0_score_loss: 0.0001, layer_0_mask_bce_loss: 0.0157, layer_0_mask_dice_loss: 0.1470, layer_0_sem_loss: 3.4510, layer_0_indi_loss: 2.2485, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0188, layer_1_mask_dice_loss: 0.1531, layer_1_sem_loss: 3.4264, layer_1_indi_loss: 2.1362, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0186, layer_2_mask_dice_loss: 0.1534, layer_2_sem_loss: 3.4416, layer_2_indi_loss: 2.1297, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0204, layer_3_mask_dice_loss: 0.1554, layer_3_sem_loss: 3.4240, layer_3_indi_loss: 2.1350, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0219, layer_4_mask_dice_loss: 0.1577, layer_4_sem_loss: 3.4119, layer_4_indi_loss: 2.1259, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0215, layer_5_mask_dice_loss: 0.1584, layer_5_sem_loss: 3.4525, layer_5_indi_loss: 2.1274, loss: 3.6630, grad_total_norm: 14.0557
2025-04-26 16:30:57,723 - INFO - Epoch [1/70][110/817]  lr: 0.0001, eta: 3 days, 1:07:16, data_time: 0.13, iter_time: 4.61, score_loss: 0.0000, mask_bce_loss: 0.0152, mask_dice_loss: 0.1151, sem_loss: 3.4721, indi_loss: 1.7729, sample_loss: 0.1446, layer_0_score_loss: 0.0009, layer_0_mask_bce_loss: 0.0109, layer_0_mask_dice_loss: 0.1037, layer_0_sem_loss: 3.3562, layer_0_indi_loss: 1.8035, layer_1_score_loss: 0.0001, layer_1_mask_bce_loss: 0.0150, layer_1_mask_dice_loss: 0.1124, layer_1_sem_loss: 3.4514, layer_1_indi_loss: 1.7798, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0136, layer_2_mask_dice_loss: 0.1115, layer_2_sem_loss: 3.4998, layer_2_indi_loss: 1.7779, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0145, layer_3_mask_dice_loss: 0.1126, layer_3_sem_loss: 3.4686, layer_3_indi_loss: 1.7771, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0142, layer_4_mask_dice_loss: 0.1131, layer_4_sem_loss: 3.4309, layer_4_indi_loss: 1.7750, layer_5_score_loss: 0.0001, layer_5_mask_bce_loss: 0.0145, layer_5_mask_dice_loss: 0.1143, layer_5_sem_loss: 3.4549, layer_5_indi_loss: 1.7736, loss: 3.3277, grad_total_norm: 23.1094
2025-04-26 16:31:42,381 - INFO - Epoch [1/70][120/817]  lr: 0.0001, eta: 3 days, 0:54:56, data_time: 0.12, iter_time: 4.60, score_loss: 0.0000, mask_bce_loss: 0.0159, mask_dice_loss: 0.1019, sem_loss: 2.7727, indi_loss: 1.7125, sample_loss: 0.1444, layer_0_score_loss: 0.0001, layer_0_mask_bce_loss: 0.0124, layer_0_mask_dice_loss: 0.0878, layer_0_sem_loss: 2.7798, layer_0_indi_loss: 1.7307, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0142, layer_1_mask_dice_loss: 0.0960, layer_1_sem_loss: 2.7626, layer_1_indi_loss: 1.7045, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0142, layer_2_mask_dice_loss: 0.0959, layer_2_sem_loss: 2.7845, layer_2_indi_loss: 1.7044, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0146, layer_3_mask_dice_loss: 0.0978, layer_3_sem_loss: 2.7719, layer_3_indi_loss: 1.7028, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0163, layer_4_mask_dice_loss: 0.1009, layer_4_sem_loss: 2.7641, layer_4_indi_loss: 1.7060, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0157, layer_5_mask_dice_loss: 0.1007, layer_5_sem_loss: 2.7830, layer_5_indi_loss: 1.7108, loss: 2.9736, grad_total_norm: 16.2032
