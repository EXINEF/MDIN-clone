Using backend: pytorch
2025-04-26 16:11:07,712 - INFO - config: configs/default_3dgres.yaml
2025-04-26 16:11:07,718 - INFO - set random seed: 1999
2025-04-26 16:11:11,117 - INFO - Parameters: 89.32M
2025-04-26 16:11:11,120 - INFO - Load pretrain from backbones/sp_unet_backbone.pth
2025-04-26 16:11:11,204 - WARNING - The model and loaded state dict do not match exactly

size mismatch for criterion.loss_weight: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([6]).
unexpected key in source state_dict: lang_proj.weight, lang_proj.bias, stm.x_mask.0.weight, stm.x_mask.0.bias, stm.x_mask.2.weight, stm.x_mask.2.bias

missing keys in source state_dict: bert_encoder.text_model.embeddings.token_embedding.weight, bert_encoder.text_model.embeddings.position_embedding.weight, bert_encoder.text_model.encoder.layers.0.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.0.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.0.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.0.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.0.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.0.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.0.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.0.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.0.layer_norm1.weight, bert_encoder.text_model.encoder.layers.0.layer_norm1.bias, bert_encoder.text_model.encoder.layers.0.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.0.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.0.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.0.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.0.layer_norm2.weight, bert_encoder.text_model.encoder.layers.0.layer_norm2.bias, bert_encoder.text_model.encoder.layers.1.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.1.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.1.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.1.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.1.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.1.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.1.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.1.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.1.layer_norm1.weight, bert_encoder.text_model.encoder.layers.1.layer_norm1.bias, bert_encoder.text_model.encoder.layers.1.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.1.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.1.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.1.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.1.layer_norm2.weight, bert_encoder.text_model.encoder.layers.1.layer_norm2.bias, bert_encoder.text_model.encoder.layers.2.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.2.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.2.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.2.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.2.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.2.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.2.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.2.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.2.layer_norm1.weight, bert_encoder.text_model.encoder.layers.2.layer_norm1.bias, bert_encoder.text_model.encoder.layers.2.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.2.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.2.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.2.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.2.layer_norm2.weight, bert_encoder.text_model.encoder.layers.2.layer_norm2.bias, bert_encoder.text_model.encoder.layers.3.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.3.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.3.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.3.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.3.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.3.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.3.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.3.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.3.layer_norm1.weight, bert_encoder.text_model.encoder.layers.3.layer_norm1.bias, bert_encoder.text_model.encoder.layers.3.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.3.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.3.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.3.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.3.layer_norm2.weight, bert_encoder.text_model.encoder.layers.3.layer_norm2.bias, bert_encoder.text_model.encoder.layers.4.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.4.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.4.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.4.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.4.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.4.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.4.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.4.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.4.layer_norm1.weight, bert_encoder.text_model.encoder.layers.4.layer_norm1.bias, bert_encoder.text_model.encoder.layers.4.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.4.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.4.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.4.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.4.layer_norm2.weight, bert_encoder.text_model.encoder.layers.4.layer_norm2.bias, bert_encoder.text_model.encoder.layers.5.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.5.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.5.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.5.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.5.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.5.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.5.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.5.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.5.layer_norm1.weight, bert_encoder.text_model.encoder.layers.5.layer_norm1.bias, bert_encoder.text_model.encoder.layers.5.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.5.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.5.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.5.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.5.layer_norm2.weight, bert_encoder.text_model.encoder.layers.5.layer_norm2.bias, bert_encoder.text_model.encoder.layers.6.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.6.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.6.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.6.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.6.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.6.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.6.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.6.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.6.layer_norm1.weight, bert_encoder.text_model.encoder.layers.6.layer_norm1.bias, bert_encoder.text_model.encoder.layers.6.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.6.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.6.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.6.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.6.layer_norm2.weight, bert_encoder.text_model.encoder.layers.6.layer_norm2.bias, bert_encoder.text_model.encoder.layers.7.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.7.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.7.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.7.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.7.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.7.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.7.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.7.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.7.layer_norm1.weight, bert_encoder.text_model.encoder.layers.7.layer_norm1.bias, bert_encoder.text_model.encoder.layers.7.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.7.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.7.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.7.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.7.layer_norm2.weight, bert_encoder.text_model.encoder.layers.7.layer_norm2.bias, bert_encoder.text_model.encoder.layers.8.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.8.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.8.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.8.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.8.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.8.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.8.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.8.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.8.layer_norm1.weight, bert_encoder.text_model.encoder.layers.8.layer_norm1.bias, bert_encoder.text_model.encoder.layers.8.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.8.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.8.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.8.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.8.layer_norm2.weight, bert_encoder.text_model.encoder.layers.8.layer_norm2.bias, bert_encoder.text_model.encoder.layers.9.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.9.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.9.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.9.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.9.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.9.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.9.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.9.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.9.layer_norm1.weight, bert_encoder.text_model.encoder.layers.9.layer_norm1.bias, bert_encoder.text_model.encoder.layers.9.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.9.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.9.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.9.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.9.layer_norm2.weight, bert_encoder.text_model.encoder.layers.9.layer_norm2.bias, bert_encoder.text_model.encoder.layers.10.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.10.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.10.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.10.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.10.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.10.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.10.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.10.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.10.layer_norm1.weight, bert_encoder.text_model.encoder.layers.10.layer_norm1.bias, bert_encoder.text_model.encoder.layers.10.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.10.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.10.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.10.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.10.layer_norm2.weight, bert_encoder.text_model.encoder.layers.10.layer_norm2.bias, bert_encoder.text_model.encoder.layers.11.self_attn.k_proj.weight, bert_encoder.text_model.encoder.layers.11.self_attn.k_proj.bias, bert_encoder.text_model.encoder.layers.11.self_attn.v_proj.weight, bert_encoder.text_model.encoder.layers.11.self_attn.v_proj.bias, bert_encoder.text_model.encoder.layers.11.self_attn.q_proj.weight, bert_encoder.text_model.encoder.layers.11.self_attn.q_proj.bias, bert_encoder.text_model.encoder.layers.11.self_attn.out_proj.weight, bert_encoder.text_model.encoder.layers.11.self_attn.out_proj.bias, bert_encoder.text_model.encoder.layers.11.layer_norm1.weight, bert_encoder.text_model.encoder.layers.11.layer_norm1.bias, bert_encoder.text_model.encoder.layers.11.mlp.fc1.weight, bert_encoder.text_model.encoder.layers.11.mlp.fc1.bias, bert_encoder.text_model.encoder.layers.11.mlp.fc2.weight, bert_encoder.text_model.encoder.layers.11.mlp.fc2.bias, bert_encoder.text_model.encoder.layers.11.layer_norm2.weight, bert_encoder.text_model.encoder.layers.11.layer_norm2.bias, bert_encoder.text_model.final_layer_norm.weight, bert_encoder.text_model.final_layer_norm.bias, mdin.input_proj.0.weight, mdin.input_proj.0.bias, mdin.input_proj.1.weight, mdin.input_proj.1.bias, mdin.lang_proj.weight, mdin.lang_proj.bias, mdin.lang_norm.weight, mdin.lang_norm.bias, mdin.sampling_module.camap.query_proj.weight, mdin.sampling_module.camap.key_proj.weight, mdin.query_generator.0.weight, mdin.query_generator.0.bias, mdin.query_generator.2.weight, mdin.query_generator.2.bias, mdin.query_generator.4.weight, mdin.query_generator.4.bias, mdin.qsa_layers.0.attn.in_proj_weight, mdin.qsa_layers.0.attn.in_proj_bias, mdin.qsa_layers.0.attn.out_proj.weight, mdin.qsa_layers.0.attn.out_proj.bias, mdin.qsa_layers.0.norm.weight, mdin.qsa_layers.0.norm.bias, mdin.qsa_layers.1.attn.in_proj_weight, mdin.qsa_layers.1.attn.in_proj_bias, mdin.qsa_layers.1.attn.out_proj.weight, mdin.qsa_layers.1.attn.out_proj.bias, mdin.qsa_layers.1.norm.weight, mdin.qsa_layers.1.norm.bias, mdin.qsa_layers.2.attn.in_proj_weight, mdin.qsa_layers.2.attn.in_proj_bias, mdin.qsa_layers.2.attn.out_proj.weight, mdin.qsa_layers.2.attn.out_proj.bias, mdin.qsa_layers.2.norm.weight, mdin.qsa_layers.2.norm.bias, mdin.qsa_layers.3.attn.in_proj_weight, mdin.qsa_layers.3.attn.in_proj_bias, mdin.qsa_layers.3.attn.out_proj.weight, mdin.qsa_layers.3.attn.out_proj.bias, mdin.qsa_layers.3.norm.weight, mdin.qsa_layers.3.norm.bias, mdin.qsa_layers.4.attn.in_proj_weight, mdin.qsa_layers.4.attn.in_proj_bias, mdin.qsa_layers.4.attn.out_proj.weight, mdin.qsa_layers.4.attn.out_proj.bias, mdin.qsa_layers.4.norm.weight, mdin.qsa_layers.4.norm.bias, mdin.qsa_layers.5.attn.in_proj_weight, mdin.qsa_layers.5.attn.in_proj_bias, mdin.qsa_layers.5.attn.out_proj.weight, mdin.qsa_layers.5.attn.out_proj.bias, mdin.qsa_layers.5.norm.weight, mdin.qsa_layers.5.norm.bias, mdin.qqa_layers.0.attn.in_proj_weight, mdin.qqa_layers.0.attn.in_proj_bias, mdin.qqa_layers.0.attn.out_proj.weight, mdin.qqa_layers.0.attn.out_proj.bias, mdin.qqa_layers.0.norm.weight, mdin.qqa_layers.0.norm.bias, mdin.qqa_layers.1.attn.in_proj_weight, mdin.qqa_layers.1.attn.in_proj_bias, mdin.qqa_layers.1.attn.out_proj.weight, mdin.qqa_layers.1.attn.out_proj.bias, mdin.qqa_layers.1.norm.weight, mdin.qqa_layers.1.norm.bias, mdin.qqa_layers.2.attn.in_proj_weight, mdin.qqa_layers.2.attn.in_proj_bias, mdin.qqa_layers.2.attn.out_proj.weight, mdin.qqa_layers.2.attn.out_proj.bias, mdin.qqa_layers.2.norm.weight, mdin.qqa_layers.2.norm.bias, mdin.qqa_layers.3.attn.in_proj_weight, mdin.qqa_layers.3.attn.in_proj_bias, mdin.qqa_layers.3.attn.out_proj.weight, mdin.qqa_layers.3.attn.out_proj.bias, mdin.qqa_layers.3.norm.weight, mdin.qqa_layers.3.norm.bias, mdin.qqa_layers.4.attn.in_proj_weight, mdin.qqa_layers.4.attn.in_proj_bias, mdin.qqa_layers.4.attn.out_proj.weight, mdin.qqa_layers.4.attn.out_proj.bias, mdin.qqa_layers.4.norm.weight, mdin.qqa_layers.4.norm.bias, mdin.qqa_layers.5.attn.in_proj_weight, mdin.qqa_layers.5.attn.in_proj_bias, mdin.qqa_layers.5.attn.out_proj.weight, mdin.qqa_layers.5.attn.out_proj.bias, mdin.qqa_layers.5.norm.weight, mdin.qqa_layers.5.norm.bias, mdin.qla_layers.0.attn.in_proj_weight, mdin.qla_layers.0.attn.in_proj_bias, mdin.qla_layers.0.attn.out_proj.weight, mdin.qla_layers.0.attn.out_proj.bias, mdin.qla_layers.0.norm.weight, mdin.qla_layers.0.norm.bias, mdin.qla_layers.1.attn.in_proj_weight, mdin.qla_layers.1.attn.in_proj_bias, mdin.qla_layers.1.attn.out_proj.weight, mdin.qla_layers.1.attn.out_proj.bias, mdin.qla_layers.1.norm.weight, mdin.qla_layers.1.norm.bias, mdin.qla_layers.2.attn.in_proj_weight, mdin.qla_layers.2.attn.in_proj_bias, mdin.qla_layers.2.attn.out_proj.weight, mdin.qla_layers.2.attn.out_proj.bias, mdin.qla_layers.2.norm.weight, mdin.qla_layers.2.norm.bias, mdin.qla_layers.3.attn.in_proj_weight, mdin.qla_layers.3.attn.in_proj_bias, mdin.qla_layers.3.attn.out_proj.weight, mdin.qla_layers.3.attn.out_proj.bias, mdin.qla_layers.3.norm.weight, mdin.qla_layers.3.norm.bias, mdin.qla_layers.4.attn.in_proj_weight, mdin.qla_layers.4.attn.in_proj_bias, mdin.qla_layers.4.attn.out_proj.weight, mdin.qla_layers.4.attn.out_proj.bias, mdin.qla_layers.4.norm.weight, mdin.qla_layers.4.norm.bias, mdin.qla_layers.5.attn.in_proj_weight, mdin.qla_layers.5.attn.in_proj_bias, mdin.qla_layers.5.attn.out_proj.weight, mdin.qla_layers.5.attn.out_proj.bias, mdin.qla_layers.5.norm.weight, mdin.qla_layers.5.norm.bias, mdin.qla_ffn_layers.0.net.0.weight, mdin.qla_ffn_layers.0.net.0.bias, mdin.qla_ffn_layers.0.net.3.weight, mdin.qla_ffn_layers.0.net.3.bias, mdin.qla_ffn_layers.0.norm.weight, mdin.qla_ffn_layers.0.norm.bias, mdin.qla_ffn_layers.1.net.0.weight, mdin.qla_ffn_layers.1.net.0.bias, mdin.qla_ffn_layers.1.net.3.weight, mdin.qla_ffn_layers.1.net.3.bias, mdin.qla_ffn_layers.1.norm.weight, mdin.qla_ffn_layers.1.norm.bias, mdin.qla_ffn_layers.2.net.0.weight, mdin.qla_ffn_layers.2.net.0.bias, mdin.qla_ffn_layers.2.net.3.weight, mdin.qla_ffn_layers.2.net.3.bias, mdin.qla_ffn_layers.2.norm.weight, mdin.qla_ffn_layers.2.norm.bias, mdin.qla_ffn_layers.3.net.0.weight, mdin.qla_ffn_layers.3.net.0.bias, mdin.qla_ffn_layers.3.net.3.weight, mdin.qla_ffn_layers.3.net.3.bias, mdin.qla_ffn_layers.3.norm.weight, mdin.qla_ffn_layers.3.norm.bias, mdin.qla_ffn_layers.4.net.0.weight, mdin.qla_ffn_layers.4.net.0.bias, mdin.qla_ffn_layers.4.net.3.weight, mdin.qla_ffn_layers.4.net.3.bias, mdin.qla_ffn_layers.4.norm.weight, mdin.qla_ffn_layers.4.norm.bias, mdin.qla_ffn_layers.5.net.0.weight, mdin.qla_ffn_layers.5.net.0.bias, mdin.qla_ffn_layers.5.net.3.weight, mdin.qla_ffn_layers.5.net.3.bias, mdin.qla_ffn_layers.5.norm.weight, mdin.qla_ffn_layers.5.norm.bias, mdin.lla_layers.0.attn.in_proj_weight, mdin.lla_layers.0.attn.in_proj_bias, mdin.lla_layers.0.attn.out_proj.weight, mdin.lla_layers.0.attn.out_proj.bias, mdin.lla_layers.0.norm.weight, mdin.lla_layers.0.norm.bias, mdin.lla_layers.1.attn.in_proj_weight, mdin.lla_layers.1.attn.in_proj_bias, mdin.lla_layers.1.attn.out_proj.weight, mdin.lla_layers.1.attn.out_proj.bias, mdin.lla_layers.1.norm.weight, mdin.lla_layers.1.norm.bias, mdin.lla_layers.2.attn.in_proj_weight, mdin.lla_layers.2.attn.in_proj_bias, mdin.lla_layers.2.attn.out_proj.weight, mdin.lla_layers.2.attn.out_proj.bias, mdin.lla_layers.2.norm.weight, mdin.lla_layers.2.norm.bias, mdin.lla_layers.3.attn.in_proj_weight, mdin.lla_layers.3.attn.in_proj_bias, mdin.lla_layers.3.attn.out_proj.weight, mdin.lla_layers.3.attn.out_proj.bias, mdin.lla_layers.3.norm.weight, mdin.lla_layers.3.norm.bias, mdin.lla_layers.4.attn.in_proj_weight, mdin.lla_layers.4.attn.in_proj_bias, mdin.lla_layers.4.attn.out_proj.weight, mdin.lla_layers.4.attn.out_proj.bias, mdin.lla_layers.4.norm.weight, mdin.lla_layers.4.norm.bias, mdin.lla_layers.5.attn.in_proj_weight, mdin.lla_layers.5.attn.in_proj_bias, mdin.lla_layers.5.attn.out_proj.weight, mdin.lla_layers.5.attn.out_proj.bias, mdin.lla_layers.5.norm.weight, mdin.lla_layers.5.norm.bias, mdin.lsa_layers.0.attn.in_proj_weight, mdin.lsa_layers.0.attn.in_proj_bias, mdin.lsa_layers.0.attn.out_proj.weight, mdin.lsa_layers.0.attn.out_proj.bias, mdin.lsa_layers.0.norm.weight, mdin.lsa_layers.0.norm.bias, mdin.lsa_layers.1.attn.in_proj_weight, mdin.lsa_layers.1.attn.in_proj_bias, mdin.lsa_layers.1.attn.out_proj.weight, mdin.lsa_layers.1.attn.out_proj.bias, mdin.lsa_layers.1.norm.weight, mdin.lsa_layers.1.norm.bias, mdin.lsa_layers.2.attn.in_proj_weight, mdin.lsa_layers.2.attn.in_proj_bias, mdin.lsa_layers.2.attn.out_proj.weight, mdin.lsa_layers.2.attn.out_proj.bias, mdin.lsa_layers.2.norm.weight, mdin.lsa_layers.2.norm.bias, mdin.lsa_layers.3.attn.in_proj_weight, mdin.lsa_layers.3.attn.in_proj_bias, mdin.lsa_layers.3.attn.out_proj.weight, mdin.lsa_layers.3.attn.out_proj.bias, mdin.lsa_layers.3.norm.weight, mdin.lsa_layers.3.norm.bias, mdin.lsa_layers.4.attn.in_proj_weight, mdin.lsa_layers.4.attn.in_proj_bias, mdin.lsa_layers.4.attn.out_proj.weight, mdin.lsa_layers.4.attn.out_proj.bias, mdin.lsa_layers.4.norm.weight, mdin.lsa_layers.4.norm.bias, mdin.lsa_layers.5.attn.in_proj_weight, mdin.lsa_layers.5.attn.in_proj_bias, mdin.lsa_layers.5.attn.out_proj.weight, mdin.lsa_layers.5.attn.out_proj.bias, mdin.lsa_layers.5.norm.weight, mdin.lsa_layers.5.norm.bias, mdin.lsa_ffn_layers.0.net.0.weight, mdin.lsa_ffn_layers.0.net.0.bias, mdin.lsa_ffn_layers.0.net.3.weight, mdin.lsa_ffn_layers.0.net.3.bias, mdin.lsa_ffn_layers.0.norm.weight, mdin.lsa_ffn_layers.0.norm.bias, mdin.lsa_ffn_layers.1.net.0.weight, mdin.lsa_ffn_layers.1.net.0.bias, mdin.lsa_ffn_layers.1.net.3.weight, mdin.lsa_ffn_layers.1.net.3.bias, mdin.lsa_ffn_layers.1.norm.weight, mdin.lsa_ffn_layers.1.norm.bias, mdin.lsa_ffn_layers.2.net.0.weight, mdin.lsa_ffn_layers.2.net.0.bias, mdin.lsa_ffn_layers.2.net.3.weight, mdin.lsa_ffn_layers.2.net.3.bias, mdin.lsa_ffn_layers.2.norm.weight, mdin.lsa_ffn_layers.2.norm.bias, mdin.lsa_ffn_layers.3.net.0.weight, mdin.lsa_ffn_layers.3.net.0.bias, mdin.lsa_ffn_layers.3.net.3.weight, mdin.lsa_ffn_layers.3.net.3.bias, mdin.lsa_ffn_layers.3.norm.weight, mdin.lsa_ffn_layers.3.norm.bias, mdin.lsa_ffn_layers.4.net.0.weight, mdin.lsa_ffn_layers.4.net.0.bias, mdin.lsa_ffn_layers.4.net.3.weight, mdin.lsa_ffn_layers.4.net.3.bias, mdin.lsa_ffn_layers.4.norm.weight, mdin.lsa_ffn_layers.4.norm.bias, mdin.lsa_ffn_layers.5.net.0.weight, mdin.lsa_ffn_layers.5.net.0.bias, mdin.lsa_ffn_layers.5.net.3.weight, mdin.lsa_ffn_layers.5.net.3.bias, mdin.lsa_ffn_layers.5.norm.weight, mdin.lsa_ffn_layers.5.norm.bias, mdin.out_norm.weight, mdin.out_norm.bias, mdin.out_score.0.weight, mdin.out_score.0.bias, mdin.out_score.2.weight, mdin.out_score.2.bias, mdin.x_mask.0.weight, mdin.x_mask.0.bias, mdin.x_mask.2.weight, mdin.x_mask.2.bias, mdin.indi_embedding.0.weight, mdin.indi_embedding.0.bias, mdin.indi_embedding.2.weight, mdin.indi_embedding.2.bias, mdin.indi_embedding.3.weight, mdin.indi_embedding.3.bias, mdin.indi_norm.weight, mdin.indi_norm.bias, mdin.contrastive_align_projection_vision.0.weight, mdin.contrastive_align_projection_vision.0.bias, mdin.contrastive_align_projection_vision.2.weight, mdin.contrastive_align_projection_vision.2.bias, mdin.contrastive_align_projection_vision.4.weight, mdin.contrastive_align_projection_vision.4.bias, mdin.contrastive_align_projection_text.0.weight, mdin.contrastive_align_projection_text.0.bias, mdin.contrastive_align_projection_text.2.weight, mdin.contrastive_align_projection_text.2.bias, mdin.contrastive_align_projection_text.4.weight, mdin.contrastive_align_projection_text.4.bias, criterion.matcher.cost_weight

2025-04-26 16:11:11,207 - INFO - Train batch size per gpu: 7
2025-04-26 16:11:11,443 - INFO - Load train multi3drefer: 43838 samples
2025-04-26 16:11:15,655 - INFO - Load val multi3drefer: 11120 samples
2025-04-26 16:11:16,926 - INFO - Training
tools/train_3dgres.py:85: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with torch.autograd.detect_anomaly():
2025-04-26 16:12:20,436 - INFO - Epoch [1/70][10/817]  lr: 0.0001, eta: 4 days, 4:51:35, data_time: 1.53, iter_time: 6.35, score_loss: 0.0000, mask_bce_loss: 0.0326, mask_dice_loss: 0.1164, sem_loss: 5.0437, indi_loss: 1.8197, sample_loss: 0.1456, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0248, layer_0_mask_dice_loss: 0.1212, layer_0_sem_loss: 5.1885, layer_0_indi_loss: 2.2621, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0246, layer_1_mask_dice_loss: 0.1217, layer_1_sem_loss: 5.0341, layer_1_indi_loss: 1.7696, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0212, layer_2_mask_dice_loss: 0.1182, layer_2_sem_loss: 5.0019, layer_2_indi_loss: 1.8224, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0225, layer_3_mask_dice_loss: 0.1176, layer_3_sem_loss: 4.9880, layer_3_indi_loss: 1.8450, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0602, layer_4_mask_dice_loss: 0.1163, layer_4_sem_loss: 4.9961, layer_4_indi_loss: 1.8580, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0398, layer_5_mask_dice_loss: 0.1170, layer_5_sem_loss: 4.9955, layer_5_indi_loss: 1.8005, loss: 4.0822, grad_total_norm: 7.8413
2025-04-26 16:13:06,597 - INFO - Epoch [1/70][20/817]  lr: 0.0001, eta: 3 days, 15:04:30, data_time: 0.76, iter_time: 5.48, score_loss: 0.0000, mask_bce_loss: 0.0319, mask_dice_loss: 0.1382, sem_loss: 5.3797, indi_loss: 2.0367, sample_loss: 0.1441, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0239, layer_0_mask_dice_loss: 0.1394, layer_0_sem_loss: 5.6723, layer_0_indi_loss: 2.6601, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0221, layer_1_mask_dice_loss: 0.1416, layer_1_sem_loss: 5.3880, layer_1_indi_loss: 1.9441, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0281, layer_2_mask_dice_loss: 0.1389, layer_2_sem_loss: 5.4222, layer_2_indi_loss: 1.9360, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0222, layer_3_mask_dice_loss: 0.1394, layer_3_sem_loss: 5.3509, layer_3_indi_loss: 1.9496, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0280, layer_4_mask_dice_loss: 0.1388, layer_4_sem_loss: 5.3975, layer_4_indi_loss: 1.9709, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0333, layer_5_mask_dice_loss: 0.1380, layer_5_sem_loss: 5.3770, layer_5_indi_loss: 2.0521, loss: 4.3561, grad_total_norm: 10.8228
2025-04-26 16:13:51,479 - INFO - Epoch [1/70][30/817]  lr: 0.0001, eta: 3 days, 9:47:36, data_time: 0.51, iter_time: 5.15, score_loss: 0.0000, mask_bce_loss: 0.0095, mask_dice_loss: 0.0531, sem_loss: 2.1718, indi_loss: 1.4533, sample_loss: 0.1387, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0107, layer_0_mask_dice_loss: 0.0529, layer_0_sem_loss: 2.3493, layer_0_indi_loss: 1.2288, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0114, layer_1_mask_dice_loss: 0.0536, layer_1_sem_loss: 2.2477, layer_1_indi_loss: 1.3563, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0106, layer_2_mask_dice_loss: 0.0533, layer_2_sem_loss: 2.1921, layer_2_indi_loss: 1.4280, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0102, layer_3_mask_dice_loss: 0.0533, layer_3_sem_loss: 2.1677, layer_3_indi_loss: 1.4191, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0100, layer_4_mask_dice_loss: 0.0534, layer_4_sem_loss: 2.1827, layer_4_indi_loss: 1.4230, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0096, layer_5_mask_dice_loss: 0.0530, layer_5_sem_loss: 2.1807, layer_5_indi_loss: 1.4162, loss: 2.3890, grad_total_norm: 7.7821
2025-04-26 16:14:34,164 - INFO - Epoch [1/70][40/817]  lr: 0.0001, eta: 3 days, 6:16:32, data_time: 0.38, iter_time: 4.93, score_loss: 0.0000, mask_bce_loss: 0.0166, mask_dice_loss: 0.0905, sem_loss: 3.3061, indi_loss: 1.6432, sample_loss: 0.1448, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0168, layer_0_mask_dice_loss: 0.0900, layer_0_sem_loss: 3.2985, layer_0_indi_loss: 1.9054, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0218, layer_1_mask_dice_loss: 0.0918, layer_1_sem_loss: 3.3276, layer_1_indi_loss: 1.6177, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0212, layer_2_mask_dice_loss: 0.0910, layer_2_sem_loss: 3.3194, layer_2_indi_loss: 1.6398, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0180, layer_3_mask_dice_loss: 0.0903, layer_3_sem_loss: 3.3057, layer_3_indi_loss: 1.6382, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0161, layer_4_mask_dice_loss: 0.0896, layer_4_sem_loss: 3.3119, layer_4_indi_loss: 1.6343, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0173, layer_5_mask_dice_loss: 0.0894, layer_5_sem_loss: 3.3052, layer_5_indi_loss: 1.6331, loss: 3.1380, grad_total_norm: 4.8751
2025-04-26 16:15:17,102 - INFO - Epoch [1/70][50/817]  lr: 0.0001, eta: 3 days, 4:14:21, data_time: 0.31, iter_time: 4.80, score_loss: 0.0001, mask_bce_loss: 0.0135, mask_dice_loss: 0.1116, sem_loss: 3.0624, indi_loss: 1.7127, sample_loss: 0.1449, layer_0_score_loss: 0.0006, layer_0_mask_bce_loss: 0.0158, layer_0_mask_dice_loss: 0.1097, layer_0_sem_loss: 3.1301, layer_0_indi_loss: 2.0566, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0162, layer_1_mask_dice_loss: 0.1135, layer_1_sem_loss: 3.0792, layer_1_indi_loss: 1.7108, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0172, layer_2_mask_dice_loss: 0.1119, layer_2_sem_loss: 3.0630, layer_2_indi_loss: 1.7189, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0162, layer_3_mask_dice_loss: 0.1122, layer_3_sem_loss: 3.0515, layer_3_indi_loss: 1.7057, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0162, layer_4_mask_dice_loss: 0.1119, layer_4_sem_loss: 3.0799, layer_4_indi_loss: 1.7063, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0166, layer_5_mask_dice_loss: 0.1121, layer_5_sem_loss: 3.0505, layer_5_indi_loss: 1.7139, loss: 3.1506, grad_total_norm: 7.2575
2025-04-26 16:16:01,405 - INFO - Epoch [1/70][60/817]  lr: 0.0001, eta: 3 days, 3:14:23, data_time: 0.26, iter_time: 4.74, score_loss: 0.0000, mask_bce_loss: 0.0153, mask_dice_loss: 0.1504, sem_loss: 4.5510, indi_loss: 2.1389, sample_loss: 0.1465, layer_0_score_loss: 0.0003, layer_0_mask_bce_loss: 0.0139, layer_0_mask_dice_loss: 0.1423, layer_0_sem_loss: 4.6358, layer_0_indi_loss: 2.8695, layer_1_score_loss: 0.0004, layer_1_mask_bce_loss: 0.0165, layer_1_mask_dice_loss: 0.1481, layer_1_sem_loss: 4.5611, layer_1_indi_loss: 2.0894, layer_2_score_loss: 0.0002, layer_2_mask_bce_loss: 0.0162, layer_2_mask_dice_loss: 0.1469, layer_2_sem_loss: 4.5560, layer_2_indi_loss: 2.0711, layer_3_score_loss: 0.0004, layer_3_mask_bce_loss: 0.0163, layer_3_mask_dice_loss: 0.1467, layer_3_sem_loss: 4.5463, layer_3_indi_loss: 2.1066, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0157, layer_4_mask_dice_loss: 0.1476, layer_4_sem_loss: 4.5597, layer_4_indi_loss: 2.1139, layer_5_score_loss: 0.0006, layer_5_mask_bce_loss: 0.0172, layer_5_mask_dice_loss: 0.1467, layer_5_sem_loss: 4.5597, layer_5_indi_loss: 2.1234, loss: 4.0687, grad_total_norm: 4.5451
2025-04-26 16:16:42,549 - INFO - Epoch [1/70][70/817]  lr: 0.0001, eta: 3 days, 1:48:21, data_time: 0.22, iter_time: 4.65, score_loss: 0.0000, mask_bce_loss: 0.0197, mask_dice_loss: 0.1293, sem_loss: 3.5118, indi_loss: 1.8760, sample_loss: 0.1446, layer_0_score_loss: 0.0000, layer_0_mask_bce_loss: 0.0219, layer_0_mask_dice_loss: 0.1217, layer_0_sem_loss: 3.5898, layer_0_indi_loss: 2.4102, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0216, layer_1_mask_dice_loss: 0.1267, layer_1_sem_loss: 3.5058, layer_1_indi_loss: 1.8819, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0233, layer_2_mask_dice_loss: 0.1263, layer_2_sem_loss: 3.5008, layer_2_indi_loss: 1.8695, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0195, layer_3_mask_dice_loss: 0.1264, layer_3_sem_loss: 3.4756, layer_3_indi_loss: 1.8700, layer_4_score_loss: 0.0002, layer_4_mask_bce_loss: 0.0193, layer_4_mask_dice_loss: 0.1273, layer_4_sem_loss: 3.4835, layer_4_indi_loss: 1.8765, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0198, layer_5_mask_dice_loss: 0.1288, layer_5_sem_loss: 3.4861, layer_5_indi_loss: 1.8701, loss: 3.4705, grad_total_norm: 10.7389
2025-04-26 16:17:25,982 - INFO - Epoch [1/70][80/817]  lr: 0.0001, eta: 3 days, 1:10:50, data_time: 0.19, iter_time: 4.61, score_loss: 0.0000, mask_bce_loss: 0.0201, mask_dice_loss: 0.1807, sem_loss: 4.4505, indi_loss: 2.1186, sample_loss: 0.1456, layer_0_score_loss: 0.0001, layer_0_mask_bce_loss: 0.0156, layer_0_mask_dice_loss: 0.1685, layer_0_sem_loss: 4.6034, layer_0_indi_loss: 3.0447, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0192, layer_1_mask_dice_loss: 0.1778, layer_1_sem_loss: 4.4737, layer_1_indi_loss: 2.1487, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0184, layer_2_mask_dice_loss: 0.1778, layer_2_sem_loss: 4.4664, layer_2_indi_loss: 2.1272, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0194, layer_3_mask_dice_loss: 0.1782, layer_3_sem_loss: 4.4592, layer_3_indi_loss: 2.1275, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0193, layer_4_mask_dice_loss: 0.1786, layer_4_sem_loss: 4.4559, layer_4_indi_loss: 2.1229, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0198, layer_5_mask_dice_loss: 0.1796, layer_5_sem_loss: 4.4482, layer_5_indi_loss: 2.1180, loss: 4.1676, grad_total_norm: 6.9390
2025-04-26 16:18:11,381 - INFO - Epoch [1/70][90/817]  lr: 0.0001, eta: 3 days, 1:02:23, data_time: 0.17, iter_time: 4.60, score_loss: 0.0000, mask_bce_loss: 0.0225, mask_dice_loss: 0.1250, sem_loss: 4.3436, indi_loss: 1.9220, sample_loss: 0.1456, layer_0_score_loss: 0.0002, layer_0_mask_bce_loss: 0.0197, layer_0_mask_dice_loss: 0.1169, layer_0_sem_loss: 4.2630, layer_0_indi_loss: 2.4843, layer_1_score_loss: 0.0000, layer_1_mask_bce_loss: 0.0216, layer_1_mask_dice_loss: 0.1217, layer_1_sem_loss: 4.3149, layer_1_indi_loss: 1.9449, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0214, layer_2_mask_dice_loss: 0.1217, layer_2_sem_loss: 4.3141, layer_2_indi_loss: 1.9197, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0210, layer_3_mask_dice_loss: 0.1227, layer_3_sem_loss: 4.3223, layer_3_indi_loss: 1.9163, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0208, layer_4_mask_dice_loss: 0.1229, layer_4_sem_loss: 4.3307, layer_4_indi_loss: 1.9201, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0231, layer_5_mask_dice_loss: 0.1246, layer_5_sem_loss: 4.3283, layer_5_indi_loss: 1.9196, loss: 3.8160, grad_total_norm: 10.3778
2025-04-26 16:18:55,344 - INFO - Epoch [1/70][100/817]  lr: 0.0001, eta: 3 days, 0:41:44, data_time: 0.15, iter_time: 4.58, score_loss: 0.0000, mask_bce_loss: 0.0127, mask_dice_loss: 0.0809, sem_loss: 2.0493, indi_loss: 1.5231, sample_loss: 0.1400, layer_0_score_loss: 0.0006, layer_0_mask_bce_loss: 0.0105, layer_0_mask_dice_loss: 0.0707, layer_0_sem_loss: 2.2126, layer_0_indi_loss: 1.6935, layer_1_score_loss: 0.0001, layer_1_mask_bce_loss: 0.0115, layer_1_mask_dice_loss: 0.0763, layer_1_sem_loss: 2.0632, layer_1_indi_loss: 1.5206, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0123, layer_2_mask_dice_loss: 0.0786, layer_2_sem_loss: 2.0567, layer_2_indi_loss: 1.5121, layer_3_score_loss: 0.0000, layer_3_mask_bce_loss: 0.0123, layer_3_mask_dice_loss: 0.0791, layer_3_sem_loss: 2.0527, layer_3_indi_loss: 1.5108, layer_4_score_loss: 0.0000, layer_4_mask_bce_loss: 0.0127, layer_4_mask_dice_loss: 0.0808, layer_4_sem_loss: 2.0495, layer_4_indi_loss: 1.5149, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0119, layer_5_mask_dice_loss: 0.0804, layer_5_sem_loss: 2.0448, layer_5_indi_loss: 1.5156, loss: 2.4990, grad_total_norm: 6.6412
2025-04-26 16:19:39,140 - INFO - Epoch [1/70][110/817]  lr: 0.0001, eta: 3 days, 0:23:18, data_time: 0.14, iter_time: 4.57, score_loss: 0.0000, mask_bce_loss: 0.0127, mask_dice_loss: 0.0760, sem_loss: 2.4668, indi_loss: 1.4783, sample_loss: 0.1437, layer_0_score_loss: 0.0001, layer_0_mask_bce_loss: 0.0099, layer_0_mask_dice_loss: 0.0685, layer_0_sem_loss: 2.5108, layer_0_indi_loss: 1.6053, layer_1_score_loss: 0.0001, layer_1_mask_bce_loss: 0.0123, layer_1_mask_dice_loss: 0.0735, layer_1_sem_loss: 2.4675, layer_1_indi_loss: 1.4899, layer_2_score_loss: 0.0001, layer_2_mask_bce_loss: 0.0128, layer_2_mask_dice_loss: 0.0738, layer_2_sem_loss: 2.4691, layer_2_indi_loss: 1.4798, layer_3_score_loss: 0.0002, layer_3_mask_bce_loss: 0.0119, layer_3_mask_dice_loss: 0.0734, layer_3_sem_loss: 2.4714, layer_3_indi_loss: 1.4817, layer_4_score_loss: 0.0001, layer_4_mask_bce_loss: 0.0120, layer_4_mask_dice_loss: 0.0740, layer_4_sem_loss: 2.4773, layer_4_indi_loss: 1.4743, layer_5_score_loss: 0.0001, layer_5_mask_bce_loss: 0.0123, layer_5_mask_dice_loss: 0.0749, layer_5_sem_loss: 2.4745, layer_5_indi_loss: 1.4780, loss: 2.6476, grad_total_norm: 4.0746
2025-04-26 16:20:21,998 - INFO - Epoch [1/70][120/817]  lr: 0.0001, eta: 3 days, 0:00:22, data_time: 0.13, iter_time: 4.54, score_loss: 0.0001, mask_bce_loss: 0.0248, mask_dice_loss: 0.1325, sem_loss: 3.2110, indi_loss: 1.8461, sample_loss: 0.1467, layer_0_score_loss: 0.0002, layer_0_mask_bce_loss: 0.0179, layer_0_mask_dice_loss: 0.1202, layer_0_sem_loss: 3.2477, layer_0_indi_loss: 2.4274, layer_1_score_loss: 0.0004, layer_1_mask_bce_loss: 0.0191, layer_1_mask_dice_loss: 0.1279, layer_1_sem_loss: 3.1873, layer_1_indi_loss: 1.8604, layer_2_score_loss: 0.0002, layer_2_mask_bce_loss: 0.0207, layer_2_mask_dice_loss: 0.1273, layer_2_sem_loss: 3.1875, layer_2_indi_loss: 1.8461, layer_3_score_loss: 0.0001, layer_3_mask_bce_loss: 0.0214, layer_3_mask_dice_loss: 0.1282, layer_3_sem_loss: 3.1894, layer_3_indi_loss: 1.8391, layer_4_score_loss: 0.0001, layer_4_mask_bce_loss: 0.0211, layer_4_mask_dice_loss: 0.1297, layer_4_sem_loss: 3.1888, layer_4_indi_loss: 1.8386, layer_5_score_loss: 0.0001, layer_5_mask_bce_loss: 0.0238, layer_5_mask_dice_loss: 0.1309, layer_5_sem_loss: 3.2203, layer_5_indi_loss: 1.8337, loss: 3.3685, grad_total_norm: 10.5170
2025-04-26 16:21:06,675 - INFO - Epoch [1/70][130/817]  lr: 0.0001, eta: 2 days, 23:54:10, data_time: 0.12, iter_time: 4.54, score_loss: 0.0001, mask_bce_loss: 0.0172, mask_dice_loss: 0.0906, sem_loss: 2.0344, indi_loss: 1.5274, sample_loss: 0.1427, layer_0_score_loss: 0.0003, layer_0_mask_bce_loss: 0.0109, layer_0_mask_dice_loss: 0.0823, layer_0_sem_loss: 2.0995, layer_0_indi_loss: 1.7341, layer_1_score_loss: 0.0006, layer_1_mask_bce_loss: 0.0137, layer_1_mask_dice_loss: 0.0875, layer_1_sem_loss: 2.0500, layer_1_indi_loss: 1.5590, layer_2_score_loss: 0.0002, layer_2_mask_bce_loss: 0.0156, layer_2_mask_dice_loss: 0.0878, layer_2_sem_loss: 2.0435, layer_2_indi_loss: 1.5281, layer_3_score_loss: 0.0002, layer_3_mask_bce_loss: 0.0159, layer_3_mask_dice_loss: 0.0881, layer_3_sem_loss: 2.0372, layer_3_indi_loss: 1.5152, layer_4_score_loss: 0.0001, layer_4_mask_bce_loss: 0.0168, layer_4_mask_dice_loss: 0.0881, layer_4_sem_loss: 2.0328, layer_4_indi_loss: 1.5139, layer_5_score_loss: 0.0002, layer_5_mask_bce_loss: 0.0173, layer_5_mask_dice_loss: 0.0896, layer_5_sem_loss: 2.0311, layer_5_indi_loss: 1.5193, loss: 2.5623, grad_total_norm: 4.2346
2025-04-26 16:21:47,986 - INFO - Epoch [1/70][140/817]  lr: 0.0001, eta: 2 days, 23:25:52, data_time: 0.11, iter_time: 4.51, score_loss: 0.0000, mask_bce_loss: 0.0106, mask_dice_loss: 0.0809, sem_loss: 2.3702, indi_loss: 1.4985, sample_loss: 0.1447, layer_0_score_loss: 0.0002, layer_0_mask_bce_loss: 0.0088, layer_0_mask_dice_loss: 0.0744, layer_0_sem_loss: 2.4045, layer_0_indi_loss: 1.6783, layer_1_score_loss: 0.0001, layer_1_mask_bce_loss: 0.0099, layer_1_mask_dice_loss: 0.0781, layer_1_sem_loss: 2.3934, layer_1_indi_loss: 1.5022, layer_2_score_loss: 0.0000, layer_2_mask_bce_loss: 0.0095, layer_2_mask_dice_loss: 0.0774, layer_2_sem_loss: 2.4009, layer_2_indi_loss: 1.4678, layer_3_score_loss: 0.0001, layer_3_mask_bce_loss: 0.0091, layer_3_mask_dice_loss: 0.0774, layer_3_sem_loss: 2.4068, layer_3_indi_loss: 1.4850, layer_4_score_loss: 0.0001, layer_4_mask_bce_loss: 0.0093, layer_4_mask_dice_loss: 0.0783, layer_4_sem_loss: 2.3787, layer_4_indi_loss: 1.4881, layer_5_score_loss: 0.0000, layer_5_mask_bce_loss: 0.0102, layer_5_mask_dice_loss: 0.0793, layer_5_sem_loss: 2.3724, layer_5_indi_loss: 1.4913, loss: 2.6303, grad_total_norm: 15.9843
2025-04-26 16:22:32,398 - INFO - Epoch [1/70][150/817]  lr: 0.0001, eta: 2 days, 23:20:55, data_time: 0.10, iter_time: 4.50, score_loss: 0.0002, mask_bce_loss: 0.0121, mask_dice_loss: 0.0735, sem_loss: 1.5176, indi_loss: 1.4091, sample_loss: 0.1427, layer_0_score_loss: 0.0002, layer_0_mask_bce_loss: 0.0101, layer_0_mask_dice_loss: 0.0691, layer_0_sem_loss: 1.5709, layer_0_indi_loss: 1.6177, layer_1_score_loss: 0.0002, layer_1_mask_bce_loss: 0.0115, layer_1_mask_dice_loss: 0.0722, layer_1_sem_loss: 1.5210, layer_1_indi_loss: 1.4421, layer_2_score_loss: 0.0001, layer_2_mask_bce_loss: 0.0116, layer_2_mask_dice_loss: 0.0708, layer_2_sem_loss: 1.5202, layer_2_indi_loss: 1.3995, layer_3_score_loss: 0.0002, layer_3_mask_bce_loss: 0.0118, layer_3_mask_dice_loss: 0.0700, layer_3_sem_loss: 1.5077, layer_3_indi_loss: 1.3935, layer_4_score_loss: 0.0004, layer_4_mask_bce_loss: 0.0136, layer_4_mask_dice_loss: 0.0702, layer_4_sem_loss: 1.5187, layer_4_indi_loss: 1.3937, layer_5_score_loss: 0.0002, layer_5_mask_bce_loss: 0.0127, layer_5_mask_dice_loss: 0.0716, layer_5_sem_loss: 1.5184, layer_5_indi_loss: 1.3994, loss: 2.2213, grad_total_norm: 5.1459
2025-04-26 16:23:17,802 - INFO - Epoch [1/70][160/817]  lr: 0.0001, eta: 2 days, 23:22:24, data_time: 0.10, iter_time: 4.51, score_loss: 0.0003, mask_bce_loss: 0.0118, mask_dice_loss: 0.0988, sem_loss: 2.4295, indi_loss: 1.6951, sample_loss: 0.1451, layer_0_score_loss: 0.0002, layer_0_mask_bce_loss: 0.0122, layer_0_mask_dice_loss: 0.0957, layer_0_sem_loss: 2.4831, layer_0_indi_loss: 1.9560, layer_1_score_loss: 0.0006, layer_1_mask_bce_loss: 0.0130, layer_1_mask_dice_loss: 0.0980, layer_1_sem_loss: 2.4461, layer_1_indi_loss: 1.7223, layer_2_score_loss: 0.0003, layer_2_mask_bce_loss: 0.0123, layer_2_mask_dice_loss: 0.0970, layer_2_sem_loss: 2.4443, layer_2_indi_loss: 1.7046, layer_3_score_loss: 0.0003, layer_3_mask_bce_loss: 0.0118, layer_3_mask_dice_loss: 0.0960, layer_3_sem_loss: 2.4412, layer_3_indi_loss: 1.6956, layer_4_score_loss: 0.0004, layer_4_mask_bce_loss: 0.0126, layer_4_mask_dice_loss: 0.0961, layer_4_sem_loss: 2.4349, layer_4_indi_loss: 1.6883, layer_5_score_loss: 0.0004, layer_5_mask_bce_loss: 0.0118, layer_5_mask_dice_loss: 0.0971, layer_5_sem_loss: 2.4275, layer_5_indi_loss: 1.6998, loss: 2.8214, grad_total_norm: 2.8499
2025-04-26 16:24:00,888 - INFO - Epoch [1/70][170/817]  lr: 0.0001, eta: 2 days, 23:10:38, data_time: 0.09, iter_time: 4.49, score_loss: 0.0006, mask_bce_loss: 0.0122, mask_dice_loss: 0.1033, sem_loss: 2.6247, indi_loss: 1.7444, sample_loss: 0.1441, layer_0_score_loss: 0.0005, layer_0_mask_bce_loss: 0.0119, layer_0_mask_dice_loss: 0.0943, layer_0_sem_loss: 2.6713, layer_0_indi_loss: 2.1094, layer_1_score_loss: 0.0006, layer_1_mask_bce_loss: 0.0128, layer_1_mask_dice_loss: 0.0989, layer_1_sem_loss: 2.6476, layer_1_indi_loss: 1.7953, layer_2_score_loss: 0.0006, layer_2_mask_bce_loss: 0.0118, layer_2_mask_dice_loss: 0.0971, layer_2_sem_loss: 2.6396, layer_2_indi_loss: 1.7890, layer_3_score_loss: 0.0008, layer_3_mask_bce_loss: 0.0120, layer_3_mask_dice_loss: 0.0967, layer_3_sem_loss: 2.6288, layer_3_indi_loss: 1.7700, layer_4_score_loss: 0.0004, layer_4_mask_bce_loss: 0.0128, layer_4_mask_dice_loss: 0.1006, layer_4_sem_loss: 2.6297, layer_4_indi_loss: 1.7750, layer_5_score_loss: 0.0009, layer_5_mask_bce_loss: 0.0125, layer_5_mask_dice_loss: 0.1015, layer_5_sem_loss: 2.6281, layer_5_indi_loss: 1.7534, loss: 2.9367, grad_total_norm: 8.5537
2025-04-26 16:24:42,602 - INFO - Epoch [1/70][180/817]  lr: 0.0001, eta: 2 days, 22:52:52, data_time: 0.09, iter_time: 4.48, score_loss: 0.0016, mask_bce_loss: 0.0140, mask_dice_loss: 0.0808, sem_loss: 1.9591, indi_loss: 1.5574, sample_loss: 0.1426, layer_0_score_loss: 0.0008, layer_0_mask_bce_loss: 0.0122, layer_0_mask_dice_loss: 0.0723, layer_0_sem_loss: 1.9897, layer_0_indi_loss: 1.7178, layer_1_score_loss: 0.0005, layer_1_mask_bce_loss: 0.0147, layer_1_mask_dice_loss: 0.0777, layer_1_sem_loss: 1.9879, layer_1_indi_loss: 1.5959, layer_2_score_loss: 0.0010, layer_2_mask_bce_loss: 0.0150, layer_2_mask_dice_loss: 0.0761, layer_2_sem_loss: 1.9773, layer_2_indi_loss: 1.5768, layer_3_score_loss: 0.0008, layer_3_mask_bce_loss: 0.0149, layer_3_mask_dice_loss: 0.0781, layer_3_sem_loss: 1.9723, layer_3_indi_loss: 1.5610, layer_4_score_loss: 0.0012, layer_4_mask_bce_loss: 0.0156, layer_4_mask_dice_loss: 0.0774, layer_4_sem_loss: 1.9665, layer_4_indi_loss: 1.5519, layer_5_score_loss: 0.0014, layer_5_mask_bce_loss: 0.0148, layer_5_mask_dice_loss: 0.0788, layer_5_sem_loss: 1.9595, layer_5_indi_loss: 1.5640, loss: 2.5047, grad_total_norm: 9.7437
2025-04-26 16:25:29,183 - INFO - Epoch [1/70][190/817]  lr: 0.0001, eta: 2 days, 23:01:14, data_time: 0.08, iter_time: 4.49, score_loss: 0.0005, mask_bce_loss: 0.0116, mask_dice_loss: 0.0768, sem_loss: 2.1630, indi_loss: 1.6308, sample_loss: 0.1416, layer_0_score_loss: 0.0006, layer_0_mask_bce_loss: 0.0095, layer_0_mask_dice_loss: 0.0779, layer_0_sem_loss: 2.1920, layer_0_indi_loss: 1.8440, layer_1_score_loss: 0.0009, layer_1_mask_bce_loss: 0.0100, layer_1_mask_dice_loss: 0.0765, layer_1_sem_loss: 2.1682, layer_1_indi_loss: 1.6280, layer_2_score_loss: 0.0007, layer_2_mask_bce_loss: 0.0100, layer_2_mask_dice_loss: 0.0744, layer_2_sem_loss: 2.1573, layer_2_indi_loss: 1.6187, layer_3_score_loss: 0.0007, layer_3_mask_bce_loss: 0.0103, layer_3_mask_dice_loss: 0.0736, layer_3_sem_loss: 2.1626, layer_3_indi_loss: 1.6262, layer_4_score_loss: 0.0008, layer_4_mask_bce_loss: 0.0105, layer_4_mask_dice_loss: 0.0728, layer_4_sem_loss: 2.1681, layer_4_indi_loss: 1.6387, layer_5_score_loss: 0.0008, layer_5_mask_bce_loss: 0.0105, layer_5_mask_dice_loss: 0.0737, layer_5_sem_loss: 2.1610, layer_5_indi_loss: 1.6305, loss: 2.5731, grad_total_norm: 10.3654
2025-04-26 16:26:14,243 - INFO - Epoch [1/70][200/817]  lr: 0.0001, eta: 2 days, 23:01:27, data_time: 0.08, iter_time: 4.49, score_loss: 0.0012, mask_bce_loss: 0.0128, mask_dice_loss: 0.0773, sem_loss: 2.4868, indi_loss: 1.7099, sample_loss: 0.1441, layer_0_score_loss: 0.0006, layer_0_mask_bce_loss: 0.0106, layer_0_mask_dice_loss: 0.0776, layer_0_sem_loss: 2.4772, layer_0_indi_loss: 1.9891, layer_1_score_loss: 0.0013, layer_1_mask_bce_loss: 0.0111, layer_1_mask_dice_loss: 0.0755, layer_1_sem_loss: 2.5004, layer_1_indi_loss: 1.7138, layer_2_score_loss: 0.0014, layer_2_mask_bce_loss: 0.0125, layer_2_mask_dice_loss: 0.0733, layer_2_sem_loss: 2.4976, layer_2_indi_loss: 1.7127, layer_3_score_loss: 0.0011, layer_3_mask_bce_loss: 0.0135, layer_3_mask_dice_loss: 0.0743, layer_3_sem_loss: 2.5005, layer_3_indi_loss: 1.7214, layer_4_score_loss: 0.0013, layer_4_mask_bce_loss: 0.0140, layer_4_mask_dice_loss: 0.0730, layer_4_sem_loss: 2.4964, layer_4_indi_loss: 1.7107, layer_5_score_loss: 0.0013, layer_5_mask_bce_loss: 0.0131, layer_5_mask_dice_loss: 0.0751, layer_5_sem_loss: 2.4930, layer_5_indi_loss: 1.7122, loss: 2.7642, grad_total_norm: 7.8583
2025-04-26 16:26:58,766 - INFO - Epoch [1/70][210/817]  lr: 0.0001, eta: 2 days, 22:59:09, data_time: 0.07, iter_time: 4.48, score_loss: 0.0014, mask_bce_loss: 0.0083, mask_dice_loss: 0.0715, sem_loss: 1.5852, indi_loss: 1.4906, sample_loss: 0.1432, layer_0_score_loss: 0.0010, layer_0_mask_bce_loss: 0.0094, layer_0_mask_dice_loss: 0.0697, layer_0_sem_loss: 1.6212, layer_0_indi_loss: 1.7809, layer_1_score_loss: 0.0013, layer_1_mask_bce_loss: 0.0096, layer_1_mask_dice_loss: 0.0703, layer_1_sem_loss: 1.6068, layer_1_indi_loss: 1.5619, layer_2_score_loss: 0.0013, layer_2_mask_bce_loss: 0.0091, layer_2_mask_dice_loss: 0.0673, layer_2_sem_loss: 1.5981, layer_2_indi_loss: 1.4993, layer_3_score_loss: 0.0012, layer_3_mask_bce_loss: 0.0086, layer_3_mask_dice_loss: 0.0663, layer_3_sem_loss: 1.5891, layer_3_indi_loss: 1.4980, layer_4_score_loss: 0.0018, layer_4_mask_bce_loss: 0.0085, layer_4_mask_dice_loss: 0.0676, layer_4_sem_loss: 1.5905, layer_4_indi_loss: 1.4968, layer_5_score_loss: 0.0019, layer_5_mask_bce_loss: 0.0082, layer_5_mask_dice_loss: 0.0691, layer_5_sem_loss: 1.5909, layer_5_indi_loss: 1.4925, loss: 2.2728, grad_total_norm: 8.0333
2025-04-26 16:27:45,588 - INFO - Epoch [1/70][220/817]  lr: 0.0001, eta: 2 days, 23:06:56, data_time: 0.07, iter_time: 4.49, score_loss: 0.0008, mask_bce_loss: 0.0121, mask_dice_loss: 0.0803, sem_loss: 1.9242, indi_loss: 1.5176, sample_loss: 0.1434, layer_0_score_loss: 0.0003, layer_0_mask_bce_loss: 0.0112, layer_0_mask_dice_loss: 0.0807, layer_0_sem_loss: 1.9453, layer_0_indi_loss: 1.7153, layer_1_score_loss: 0.0006, layer_1_mask_bce_loss: 0.0123, layer_1_mask_dice_loss: 0.0807, layer_1_sem_loss: 1.9315, layer_1_indi_loss: 1.5754, layer_2_score_loss: 0.0006, layer_2_mask_bce_loss: 0.0126, layer_2_mask_dice_loss: 0.0779, layer_2_sem_loss: 1.9385, layer_2_indi_loss: 1.5391, layer_3_score_loss: 0.0006, layer_3_mask_bce_loss: 0.0130, layer_3_mask_dice_loss: 0.0777, layer_3_sem_loss: 1.9298, layer_3_indi_loss: 1.5281, layer_4_score_loss: 0.0005, layer_4_mask_bce_loss: 0.0132, layer_4_mask_dice_loss: 0.0772, layer_4_sem_loss: 1.9328, layer_4_indi_loss: 1.5252, layer_5_score_loss: 0.0004, layer_5_mask_bce_loss: 0.0128, layer_5_mask_dice_loss: 0.0788, layer_5_sem_loss: 1.9315, layer_5_indi_loss: 1.5251, loss: 2.4718, grad_total_norm: 9.0449
2025-04-26 16:28:30,148 - INFO - Epoch [1/70][230/817]  lr: 0.0001, eta: 2 days, 23:04:37, data_time: 0.07, iter_time: 4.49, score_loss: 0.0013, mask_bce_loss: 0.0127, mask_dice_loss: 0.1320, sem_loss: 3.1441, indi_loss: 1.9491, sample_loss: 0.1433, layer_0_score_loss: 0.0008, layer_0_mask_bce_loss: 0.0120, layer_0_mask_dice_loss: 0.1326, layer_0_sem_loss: 3.1671, layer_0_indi_loss: 2.4804, layer_1_score_loss: 0.0009, layer_1_mask_bce_loss: 0.0128, layer_1_mask_dice_loss: 0.1333, layer_1_sem_loss: 3.1532, layer_1_indi_loss: 1.9883, layer_2_score_loss: 0.0010, layer_2_mask_bce_loss: 0.0137, layer_2_mask_dice_loss: 0.1287, layer_2_sem_loss: 3.1377, layer_2_indi_loss: 1.9624, layer_3_score_loss: 0.0009, layer_3_mask_bce_loss: 0.0123, layer_3_mask_dice_loss: 0.1285, layer_3_sem_loss: 3.1448, layer_3_indi_loss: 1.9358, layer_4_score_loss: 0.0011, layer_4_mask_bce_loss: 0.0130, layer_4_mask_dice_loss: 0.1270, layer_4_sem_loss: 3.1466, layer_4_indi_loss: 1.9455, layer_5_score_loss: 0.0009, layer_5_mask_bce_loss: 0.0129, layer_5_mask_dice_loss: 0.1289, layer_5_sem_loss: 3.1536, layer_5_indi_loss: 1.9482, loss: 3.3376, grad_total_norm: 6.9091
2025-04-26 16:29:13,389 - INFO - Epoch [1/70][240/817]  lr: 0.0001, eta: 2 days, 22:57:13, data_time: 0.07, iter_time: 4.49, score_loss: 0.0008, mask_bce_loss: 0.0130, mask_dice_loss: 0.0889, sem_loss: 1.8664, indi_loss: 1.6782, sample_loss: 0.1441, layer_0_score_loss: 0.0009, layer_0_mask_bce_loss: 0.0113, layer_0_mask_dice_loss: 0.0902, layer_0_sem_loss: 1.8698, layer_0_indi_loss: 1.9921, layer_1_score_loss: 0.0013, layer_1_mask_bce_loss: 0.0108, layer_1_mask_dice_loss: 0.0905, layer_1_sem_loss: 1.8811, layer_1_indi_loss: 1.7825, layer_2_score_loss: 0.0011, layer_2_mask_bce_loss: 0.0113, layer_2_mask_dice_loss: 0.0866, layer_2_sem_loss: 1.8758, layer_2_indi_loss: 1.7140, layer_3_score_loss: 0.0011, layer_3_mask_bce_loss: 0.0117, layer_3_mask_dice_loss: 0.0862, layer_3_sem_loss: 1.8732, layer_3_indi_loss: 1.6967, layer_4_score_loss: 0.0010, layer_4_mask_bce_loss: 0.0127, layer_4_mask_dice_loss: 0.0866, layer_4_sem_loss: 1.8636, layer_4_indi_loss: 1.6724, layer_5_score_loss: 0.0008, layer_5_mask_bce_loss: 0.0125, layer_5_mask_dice_loss: 0.0872, layer_5_sem_loss: 1.8619, layer_5_indi_loss: 1.6748, loss: 2.5517, grad_total_norm: 4.5703
2025-04-26 16:29:59,227 - INFO - Epoch [1/70][250/817]  lr: 0.0001, eta: 2 days, 23:00:13, data_time: 0.06, iter_time: 4.49, score_loss: 0.0011, mask_bce_loss: 0.0264, mask_dice_loss: 0.1238, sem_loss: 2.9867, indi_loss: 2.0572, sample_loss: 0.1457, layer_0_score_loss: 0.0006, layer_0_mask_bce_loss: 0.0149, layer_0_mask_dice_loss: 0.1206, layer_0_sem_loss: 2.9863, layer_0_indi_loss: 2.4523, layer_1_score_loss: 0.0015, layer_1_mask_bce_loss: 0.0165, layer_1_mask_dice_loss: 0.1225, layer_1_sem_loss: 2.9791, layer_1_indi_loss: 2.0439, layer_2_score_loss: 0.0022, layer_2_mask_bce_loss: 0.0185, layer_2_mask_dice_loss: 0.1198, layer_2_sem_loss: 2.9714, layer_2_indi_loss: 2.0197, layer_3_score_loss: 0.0015, layer_3_mask_bce_loss: 0.0210, layer_3_mask_dice_loss: 0.1184, layer_3_sem_loss: 2.9901, layer_3_indi_loss: 2.0189, layer_4_score_loss: 0.0015, layer_4_mask_bce_loss: 0.0230, layer_4_mask_dice_loss: 0.1190, layer_4_sem_loss: 2.9800, layer_4_indi_loss: 2.0219, layer_5_score_loss: 0.0016, layer_5_mask_bce_loss: 0.0235, layer_5_mask_dice_loss: 0.1206, layer_5_sem_loss: 2.9828, layer_5_indi_loss: 2.0488, loss: 3.3219, grad_total_norm: 37.5821
2025-04-26 16:30:44,726 - INFO - Epoch [1/70][260/817]  lr: 0.0001, eta: 2 days, 23:01:40, data_time: 0.06, iter_time: 4.49, score_loss: 0.0012, mask_bce_loss: 0.0106, mask_dice_loss: 0.0386, sem_loss: 0.7581, indi_loss: 1.0386, sample_loss: 0.1393, layer_0_score_loss: 0.0007, layer_0_mask_bce_loss: 0.0079, layer_0_mask_dice_loss: 0.0376, layer_0_sem_loss: 0.7889, layer_0_indi_loss: 1.2214, layer_1_score_loss: 0.0007, layer_1_mask_bce_loss: 0.0089, layer_1_mask_dice_loss: 0.0369, layer_1_sem_loss: 0.7692, layer_1_indi_loss: 1.1117, layer_2_score_loss: 0.0009, layer_2_mask_bce_loss: 0.0096, layer_2_mask_dice_loss: 0.0355, layer_2_sem_loss: 0.7536, layer_2_indi_loss: 1.0332, layer_3_score_loss: 0.0010, layer_3_mask_bce_loss: 0.0103, layer_3_mask_dice_loss: 0.0361, layer_3_sem_loss: 0.7577, layer_3_indi_loss: 1.0343, layer_4_score_loss: 0.0011, layer_4_mask_bce_loss: 0.0108, layer_4_mask_dice_loss: 0.0362, layer_4_sem_loss: 0.7618, layer_4_indi_loss: 1.0372, layer_5_score_loss: 0.0014, layer_5_mask_bce_loss: 0.0111, layer_5_mask_dice_loss: 0.0367, layer_5_sem_loss: 0.7623, layer_5_indi_loss: 1.0434, loss: 1.6144, grad_total_norm: 2.5035
2025-04-26 16:31:27,527 - INFO - Epoch [1/70][270/817]  lr: 0.0001, eta: 2 days, 22:53:30, data_time: 0.06, iter_time: 4.48, score_loss: 0.0023, mask_bce_loss: 0.0104, mask_dice_loss: 0.0813, sem_loss: 2.3410, indi_loss: 1.5706, sample_loss: 0.1431, layer_0_score_loss: 0.0007, layer_0_mask_bce_loss: 0.0103, layer_0_mask_dice_loss: 0.0893, layer_0_sem_loss: 2.3817, layer_0_indi_loss: 1.9636, layer_1_score_loss: 0.0015, layer_1_mask_bce_loss: 0.0097, layer_1_mask_dice_loss: 0.0844, layer_1_sem_loss: 2.3749, layer_1_indi_loss: 1.7236, layer_2_score_loss: 0.0015, layer_2_mask_bce_loss: 0.0101, layer_2_mask_dice_loss: 0.0812, layer_2_sem_loss: 2.3536, layer_2_indi_loss: 1.6302, layer_3_score_loss: 0.0015, layer_3_mask_bce_loss: 0.0099, layer_3_mask_dice_loss: 0.0795, layer_3_sem_loss: 2.3533, layer_3_indi_loss: 1.5905, layer_4_score_loss: 0.0011, layer_4_mask_bce_loss: 0.0103, layer_4_mask_dice_loss: 0.0802, layer_4_sem_loss: 2.3600, layer_4_indi_loss: 1.5631, layer_5_score_loss: 0.0012, layer_5_mask_bce_loss: 0.0102, layer_5_mask_dice_loss: 0.0799, layer_5_sem_loss: 2.3417, layer_5_indi_loss: 1.5605, loss: 2.6678, grad_total_norm: 16.6156
